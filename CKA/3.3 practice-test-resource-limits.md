### A pod called rabbit is deployed. Identify the CPU requirements set on the Pod

in the current(default) namespace

Run the command ```kubectl describe pod rabbit``` and inspect ```requests```.

```ruby
controlplane ~ ➜  kubectl get all
NAME         READY   STATUS              RESTARTS      AGE
pod/rabbit   0/1     RunContainerError   2 (14s ago)   32s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   7m19s

controlplane ~ ➜  kubectl describe pod rabbit 
Name:             rabbit
Namespace:        default
Priority:         0
Service Account:  default
Node:             controlplane/192.15.11.9
Start Time:       Thu, 08 Feb 2024 07:37:10 +0000
Labels:           <none>
Annotations:      <none>
Status:           Running
IP:               10.42.0.9
IPs:
  IP:  10.42.0.9
Containers:
  cpu-stress:
    Container ID:  containerd://ad1c811c3023e71aa1f193b137aac72425344efc1830eb7739be9bc1ea486a71
    Image:         ubuntu
    Image ID:      docker.io/library/ubuntu@sha256:e9569c25505f33ff72e88b2990887c9dcf230f23259da296eb814fc2b41af999
    Port:          <none>
    Host Port:     <none>
    Args:
      sleep
      1000
    State:          Waiting
      Reason:       RunContainerError
    Last State:     Terminated
      Reason:       StartError
      Message:      failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: failed to write "200000": write /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod68a193f8-c7cf-4e86-a2f8-479064e7ec12/ad1c811c3023e71aa1f193b137aac72425344efc1830eb7739be9bc1ea486a71/cpu.cfs_quota_us: invalid argument: unknown
      Exit Code:    128
      Started:      Thu, 01 Jan 1970 00:00:00 +0000
      Finished:     Thu, 08 Feb 2024 07:37:58 +0000
    Ready:          False
    Restart Count:  3
    Limits:
      cpu:  2
    Requests:
      cpu:        1
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sngh9 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-sngh9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age               From               Message
  ----     ------     ----              ----               -------
  Normal   Scheduled  53s               default-scheduler  Successfully assigned default/rabbit to controlplane
  Normal   Pulled     50s               kubelet            Successfully pulled image "ubuntu" in 2.303756046s (2.303774895s including waiting)
  Normal   Pulled     49s               kubelet            Successfully pulled image "ubuntu" in 159.663411ms (159.680793ms including waiting)
  Normal   Pulled     35s               kubelet            Successfully pulled image "ubuntu" in 156.981032ms (157.007164ms including waiting)
  Normal   Pulling    5s (x4 over 52s)  kubelet            Pulling image "ubuntu"
  Normal   Pulled     5s                kubelet            Successfully pulled image "ubuntu" in 159.860832ms (159.878743ms including waiting)
  Normal   Created    5s (x4 over 50s)  kubelet            Created container cpu-stress
  Warning  Failed     5s (x4 over 50s)  kubelet            Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: failed to write "200000": write /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod68a193f8-c7cf-4e86-a2f8-479064e7ec12/cpu-stress/cpu.cfs_quota_us: invalid argument: unknown
  Warning  BackOff    4s (x5 over 48s)  kubelet            Back-off restarting failed container cpu-stress in pod rabbit_default(68a193f8-c7cf-4e86-a2f8-479064e7ec12)

controlplane ~ ➜  kubectl describe pod rabbit | grep cpu
  cpu-stress:
      Message:      failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: failed to write "200000": write /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod68a193f8-c7cf-4e86-a2f8-479064e7ec12/b2ca57c6a9e7bb7708b4a8df8d975a5038c7a4768583f4ffb0be27deef874f27/cpu.cfs_quota_us: invalid argument: unknown
      cpu:  2
      cpu:        1
  Normal   Created    2m8s (x4 over 2m53s)  kubelet            Created container cpu-stress
  Warning  Failed     2m8s (x4 over 2m53s)  kubelet            Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: failed to write "200000": write /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod68a193f8-c7cf-4e86-a2f8-479064e7ec12/cpu-stress/cpu.cfs_quota_us: invalid argument: unknown
  Warning  BackOff    90s (x8 over 2m51s)   kubelet            Back-off restarting failed container cpu-stress in pod rabbit_default(68a193f8-c7cf-4e86-a2f8-479064e7ec12)

controlplane ~ ➜  
```

### Delete the rabbit Pod.


Once deleted, wait for the pod to fully terminate.

Delete Pod rabbit
```ruby

controlplane ~ ➜  kubectl delete pod rabbit 
pod "rabbit" deleted

controlplane ~ ➜  kubectl get pod
No resources found in default namespace.
```

### Another pod called elephant has been deployed in the default namespace. It fails to get to a running state. Inspect this pod and identify the Reason why it is not running.

The reason for the pod not running is OOMKilled. This means that the pod ran out of memory.

    root@controlplane:~# kubectl describe pod elephant  | grep -A5 State: 
        State:          Waiting
          Reason:       CrashLoopBackOff
        Last State:     Terminated
          Reason:       OOMKilled
          Exit Code:    1
          Started:      Sun, 25 Apr 2021 15:13:07 +0000
          Finished:     Sun, 25 Apr 2021 15:13:07 +0000
        Ready:          False
    root@controlplane:~# 


It's changing the status frequently so make use of the watch command to get the output every two seconds: -

root@controlplane ~ ➜  ```watch kubectl get pods```
Note: - Make use of the ```CTRL + C``` key to exit from the process.



## The status OOMKilled indicates that it is failing because the pod ran out of memory. Identify the memory limit set on the POD.


---

### The elephant pod runs a process that consumes 15Mi of memory. Increase the limit of the elephant pod to 20Mi.


Delete and recreate the pod if required. Do not modify anything other than the required fields.




    Pod Name: elephant
    
    Image Name: polinux/stress
    
    Memory Limit: 20Mi

Create the file elephant.yaml by running command ```kubectl get po elephant -o yaml > elephant.yaml``` and edit the file such as memory limit is set to 20Mi as follows:

    ---
    apiVersion: v1
    kind: Pod
    metadata:
      name: elephant
      namespace: default
    spec:
      containers:
      - args:
        - --vm
        - "1"
        - --vm-bytes
        - 15M
        - --vm-hang
        - "1"
        command:
        - stress
        image: polinux/stress
        name: mem-stress
        resources:
          limits:
            memory: 20Mi
          requests:
            memory: 5Mi
            
then run ```kubectl replace -f elephant.yaml --force```. This command will delete the existing one first and recreate a new one from the YAML file.

```ruby
controlplane ~ ➜  kubectl get pod elephant -o yaml > elephant.yaml

controlplane ~ ➜  ls
elephant.yaml  sample.yaml

controlplane ~ ➜  cat elephant.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2024-02-08T07:42:58Z"
  name: elephant
  namespace: default
  resourceVersion: "1026"
  uid: ae2ed107-0949-475f-9f99-11a7e3d6db17
spec:
  containers:
  - args:
    - --vm
    - "1"
    - --vm-bytes
    - 15M
    - --vm-hang
    - "1"
    command:
    - stress
    image: polinux/stress
    imagePullPolicy: Always
    name: mem-stress
    resources:
      limits:
        memory: 10Mi
      requests:
        memory: 5Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-v8bgx
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-v8bgx
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-02-08T07:42:58Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-02-08T07:48:32Z"
    message: 'containers with unready status: [mem-stress]'
    reason: ContainersNotReady
    status: "False"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-02-08T07:48:32Z"
    message: 'containers with unready status: [mem-stress]'
    reason: ContainersNotReady
    status: "False"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-02-08T07:42:58Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://a8660523379df475d4b8918e5d18b752eb0563005b273dd5ea4a638ff96d12df
    image: docker.io/polinux/stress:latest
    imageID: docker.io/polinux/stress@sha256:b6144f84f9c15dac80deb48d3a646b55c7043ab1d83ea0a697c09097aaad21aa
    lastState:
      terminated:
        containerID: containerd://a8660523379df475d4b8918e5d18b752eb0563005b273dd5ea4a638ff96d12df
        exitCode: 1
        finishedAt: "2024-02-08T07:48:31Z"
        reason: OOMKilled
        startedAt: "2024-02-08T07:48:31Z"
    name: mem-stress
    ready: false
    restartCount: 6
    started: false
    state:
      waiting:
        message: back-off 5m0s restarting failed container=mem-stress pod=elephant_default(ae2ed107-0949-475f-9f99-11a7e3d6db17)
        reason: CrashLoopBackOff
  hostIP: 192.15.11.9
  phase: Running
  podIP: 10.42.0.10
  podIPs:
  - ip: 10.42.0.10
  qosClass: Burstable
  startTime: "2024-02-08T07:42:58Z"

controlplane ~ ➜  vi elephant.yaml 

controlplane ~ ➜  kubectl replace -f elephant.yaml 
Error from server (Conflict): error when replacing "elephant.yaml": Operation cannot be fulfilled on pods "elephant": the object has been modified; please apply your changes to the latest version and try again

controlplane ~ ✖ kubectl replace -f elephant.yaml 
Error from server (Conflict): error when replacing "elephant.yaml": Operation cannot be fulfilled on pods "elephant": the object has been modified; please apply your changes to the latest version and try again

controlplane ~ ✖ ls
elephant.yaml  sample.yaml

controlplane ~ ➜  vi elephant.yaml 

controlplane ~ ➜  kubectl replace -f elephant.yaml 
Error from server (Conflict): error when replacing "elephant.yaml": Operation cannot be fulfilled on pods "elephant": the object has been modified; please apply your changes to the latest version and try again

controlplane ~ ✖ kubectl get pod
NAME       READY   STATUS             RESTARTS        AGE
elephant   0/1     CrashLoopBackOff   7 (2m36s ago)   13m

controlplane ~ ➜  kubectl delete pod elephant 
pod "elephant" deleted

controlplane ~ ➜  kubectl apply -f elephant.yaml 
pod/elephant created

controlplane ~ ➜  kubectl get pod
NAME       READY   STATUS    RESTARTS   AGE
elephant   1/1     Running   0          7s

```

