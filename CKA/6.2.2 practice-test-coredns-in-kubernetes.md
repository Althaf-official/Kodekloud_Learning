### Identify the DNS solution implemented in this cluster.

Run the command: ```kubectl get pods -n kube-system``` and look for the DNS pods.

```ruby
controlplane ~ ➜  kubectl get pods -n kube-system 
NAME                                   READY   STATUS    RESTARTS   AGE
coredns-69f9c977-595rl                 1/1     Running   0          8m
coredns-69f9c977-wsz9m                 1/1     Running   0          8m
etcd-controlplane                      1/1     Running   0          8m15s
kube-apiserver-controlplane            1/1     Running   0          8m12s
kube-controller-manager-controlplane   1/1     Running   0          8m12s
kube-proxy-44w6b                       1/1     Running   0          8m1s
kube-scheduler-controlplane            1/1     Running   0          8m14s

controlplane ~ ➜  
```

### How many pods of the DNS server are deployed?

```ruby
controlplane ~ ➜  kubectl get pods -n kube-system | grep coredns
coredns-69f9c977-595rl                 1/1     Running   0          10m
coredns-69f9c977-wsz9m                 1/1     Running   0          10m

controlplane ~ ➜  
```

    2

### What is the name of the service created for accessing CoreDNS?

```ruby
controlplane ~ ➜  k get svc -n kube-system 
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   11m

controlplane ~ ➜  
```
    kube-dns

### What is the IP of the CoreDNS server that should be configured on PODs to resolve services?


```ruby
controlplane ~ ➜  k get svc -n kube-system 
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   11m

controlplane ~ ➜  
```
    10.96.0.10


### Where is the configuration file located for configuring the CoreDNS service?

Inspect the ```Args``` field of the coredns deployment and check the file used.

```ruby
controlplane ~ ➜  k get pod -n kube-system coredns-69f9c977-wsz9m -oyaml | grep -A10 args
  - args:
    - -conf
    - /etc/coredns/Corefile
    image: registry.k8s.io/coredns/coredns:v1.10.1
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 5
      httpGet:
        path: /health
        port: 8080
        scheme: HTTP

controlplane ~ ➜
controlplane ~ ✖ kubectl -n kube-system describe deployments.apps coredns | grep -A2 Args |
 grep Corefile
      /etc/coredns/Corefile

controlplane ~ ➜  
```


    - /etc/coredns/Corefile

### How is the Corefile passed into the CoreDNS POD?

Run the command: ```kubectl get cm -n kube-system``` and look at the coredns ConfigMap. 
It's passed through the ConfigMap volume in the deployment.


```ruby
controlplane ~ ➜  k get -n kube-system configmaps 
NAME                                                   DATA   AGE
coredns                                                1      23m
extension-apiserver-authentication                     6      23m
kube-apiserver-legacy-service-account-token-tracking   1      23m
kube-proxy                                             2      23m
kube-root-ca.crt                                       1      23m
kubeadm-config                                         1      23m
kubelet-config                                         1      23m

controlplane ~ ➜  k describe -n kube-system configmaps coredns 
Name:         coredns
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Data
====
Corefile:
----
.:53 {
    errors
    health {
       lameduck 5s
    }
    ready
    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }
    prometheus :9153
    forward . /etc/resolv.conf {
       max_concurrent 1000
    }
    cache 30
    loop
    reload
    loadbalance
}


BinaryData
====

Events:  <none>

controlplane ~ ➜  
```


    Configured as a ConfigMap object

### What is the name of the ConfigMap object created for Corefile?

```ruby
controlplane ~ ➜  k describe -n kube-system configmaps coredns 
Name:         coredns
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Data
====
Corefile:
----
.:53 {
    errors
    health {
       lameduck 5s
    }
    ready
    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }
    prometheus :9153
    forward . /etc/resolv.conf {
       max_concurrent 1000
    }
    cache 30
    loop
    reload
    loadbalance
}


BinaryData
====

Events:  <none>

controlplane ~ ➜ 
```

    coredns


### What is the root domain/zone configured for this kubernetes cluster?

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/60e95f5a-04cc-4989-892f-98999030f55e)

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/1cb11292-fab3-4303-b267-5976d3583596)


Run the command: ```kubectl describe configmap coredns -n kube-system``` and look for the entry after kubernetes.


#### We have deployed a set of PODs and Services in the default and payroll namespaces. Inspect them and go to the next question.

```ruby
controlplane ~ ➜  k get all
NAME                    READY   STATUS    RESTARTS   AGE
pod/hr                  1/1     Running   0          25m
pod/simple-webapp-1     1/1     Running   0          25m
pod/simple-webapp-122   1/1     Running   0          25m
pod/test                1/1     Running   0          25m

NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
service/kubernetes     ClusterIP   10.96.0.1        <none>        443/TCP        32m
service/test-service   NodePort    10.102.199.162   <none>        80:30080/TCP   25m
service/web-service    ClusterIP   10.98.33.218     <none>        80/TCP         25m

controlplane ~ ➜  k get all -n payroll 
NAME      READY   STATUS    RESTARTS   AGE
pod/web   1/1     Running   0          25m

NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/web-service   ClusterIP   10.109.72.204   <none>        80/TCP    25m

controlplane ~ ➜  
```

### What name can be used to access the ```hr``` web server from the test Application?

You can execute a curl command on the test pod to test. Alternatively, the test Application also has a UI. Access it using the tab at the top of your terminal named test-app

```ruby
controlplane ~ ➜  k get svc
NAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes     ClusterIP   10.96.0.1        <none>        443/TCP        35m
test-service   NodePort    10.102.199.162   <none>        80:30080/TCP   28m
web-service    ClusterIP   10.98.33.218     <none>        80/TCP         28m

controlplane ~ ➜ 
```

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/8b9c1c2c-4114-40b7-bd03-7503b776f132)

    web-service

### Which of the names CANNOT be used to access the HR service from the test pod?

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/f1e50cd0-398d-4c8f-8700-e4affa80e3ff)
  
    web-sevice.default.pod

### Which of the below name can be used to access the ```payroll``` service from the test application?

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/f5f646a5-fd2b-4ffb-ade8-9d395be81043)

    web-service.payroll

---

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/e301900b-d087-4e1d-9d0b-930d2234b16f)

    web-service.payroll.svc.cluster

### We just deployed a web server - ```webapp``` - that accesses a database ```mysql``` - server. However the web server is failing to connect to the database server. Troubleshoot and fix the issue.


They could be in different namespaces. First locate the applications. The web server interface can be seen by clicking the tab Web Server at the top of your terminal.


![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/b3a7e72a-219b-40f5-95ab-603b3441e8d7)



Set the ```DB_Host``` environment variable to use ```mysql.payroll```

Run the command: 

    kubectl edit deploy webapp 
    
and correct the ```DB_Host``` value.


```ruby
controlplane ~ ➜  k get deployments.apps webapp 
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
webapp   1/1     1            1           3m34s

controlplane ~ ➜  k describe deployments.apps webapp 
Name:                   webapp
Namespace:              default
CreationTimestamp:      Fri, 01 Mar 2024 08:34:03 +0000
Labels:                 name=webapp
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               name=webapp
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=webapp
  Containers:
   simple-webapp-mysql:
    Image:      mmumshad/simple-webapp-mysql
    Port:       8080/TCP
    Host Port:  0/TCP
    Environment:
      DB_Host:      mysql
      DB_User:      root
      DB_Password:  paswrd
    Mounts:         <none>
  Volumes:          <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   webapp-b9548974b (1/1 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  3m44s  deployment-controller  Scaled up replica set webapp-b9548974b to 1

controlplane ~ ➜  k edit deployments.apps webapp 
deployment.apps/webapp edited

controlplane ~ ➜  k describe deployments.apps webapp 
Name:                   webapp
Namespace:              default
CreationTimestamp:      Fri, 01 Mar 2024 08:34:03 +0000
Labels:                 name=webapp
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               name=webapp
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=webapp
  Containers:
   simple-webapp-mysql:
    Image:      mmumshad/simple-webapp-mysql
    Port:       8080/TCP
    Host Port:  0/TCP
    Environment:
      DB_Host:      mysql.payroll
      DB_User:      root
      DB_Password:  paswrd
    Mounts:         <none>
  Volumes:          <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  webapp-b9548974b (0/0 replicas created)
NewReplicaSet:   webapp-744595c5c5 (1/1 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  5m20s  deployment-controller  Scaled up replica set webapp-b9548974b to 1
  Normal  ScalingReplicaSet  4s     deployment-controller  Scaled up replica set webapp-744595c5c5 to 1
  Normal  ScalingReplicaSet  1s     deployment-controller  Scaled down replica set webapp-b9548974b to 0 from 1

controlplane ~ ➜  
```

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/340c4063-b960-4236-b373-fe4943942b12)

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/285b45fb-b653-4ad6-975a-1ad9c7158d82)



### From the ```hr``` pod ```nslookup``` the mysql service and redirect the output to a file ```/root/CKA/nslookup.out```


Run the command: 

    kubectl exec -it hr -- nslookup mysql.payroll > /root/CKA/nslookup.out

```ruby
controlplane ~ ➜  kubectl exec -it hr -- nslookup mysql.payroll > /root/CKA/nslookup.out

controlplane ~ ➜ 
```


The command you provided is using `kubectl` to execute a command inside a pod named "hr". It's running the `nslookup` command to look up the DNS information for "mysql.payroll" within that pod. The output of this command is being redirected to a file named "nslookup.out" located in the "/root/CKA" directory.

Here's a breakdown of the command:

- `kubectl exec -it hr`: This part of the command tells Kubernetes to execute a command within the pod named "hr". The `-it` flags are used to allocate a pseudo-TTY and keep STDIN open even if not attached.
  
- `-- nslookup mysql.payroll`: This part of the command specifies the command to execute inside the pod. It's using `nslookup` to perform a DNS lookup for the hostname "mysql.payroll".

- `> /root/CKA/nslookup.out`: This part of the command redirects the output of the `nslookup` command to a file named "nslookup.out" located in the "/root/CKA" directory. If the file doesn't exist, it will be created; if it does exist, it will be overwritten.

- 
