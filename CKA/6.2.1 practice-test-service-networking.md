#  Do the best you can. No one can do more than that.

– John Wooden

### What network range are the nodes in the cluster part of?

Run the command: ip addr and look at the IP address assigned to the eth0 interfaces. Derive network range from that.

one way to do this is to make use of the ipcalc utility. If it is not installed, you can install it by running:

    apt update
    
and the 
    
    apt install ipcalc

Then use it to determine the network range as shown below:


First, find the Internal IP of the nodes.

    root@controlplane:~> ip a | grep eth0
    93657: eth0@if93658: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default 
        inet 10.33.39.8/24 brd 10.33.39.255 scope global eth0
    
    root@controlplane:~>


Next, use the ipcalc tool to see the network details:

    root@controlplane:~> ipcalc -b 10.33.39.8
    Address:   10.33.39.8           
    Netmask:   255.255.255.0 = 24   
    Wildcard:  0.0.0.255            
    =>
    Network:   10.33.39.0/24        
    HostMin:   10.33.39.1           
    HostMax:   10.33.39.254         
    Broadcast: 10.33.39.255         
    Hosts/Net: 254                   Class A, Private Internet
    
    root@controlplane:~>


In this example, the network is ```10.33.39.0/24```. Note, this may vary for your lab so make sure to check for yourself.


![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/9fede1da-cf2c-481f-af27-0e92eda8e5e4)


```ruby
controlplane ~ ➜  ip addr | grep eth0
7560: eth0@if7561: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default 
    inet 192.23.209.12/24 brd 192.23.209.255 scope global eth0

controlplane ~ ➜  ipcalc -b 192.23.209.12
Address:   192.23.209.12        
Netmask:   255.255.255.0 = 24   
Wildcard:  0.0.0.255            
=>
Network:   192.23.209.0/24      
HostMin:   192.23.209.1         
HostMax:   192.23.209.254       
Broadcast: 192.23.209.255       
Hosts/Net: 254                   Class C


controlplane ~ ➜  
```
    192.23.209.12/24

### What is the range of IP addresses configured for PODs on this cluster?

The network is configured with ```weave```. Check the weave pods logs using the command ```kubectl logs <weave-pod-name> weave -n kube-system``` and look for ```ipalloc-range```

```ruby
controlplane ~ ✖ k logs -n kube-system weave-net-n2v85 
Defaulted container "weave" out of: weave, weave-npc, weave-init (init)
INFO: 2024/02/29 15:29:32.290384 Command line options: map[conn-limit:200 datapath:datapath db-prefix:/weavedb/weave-net docker-api: expect-npc:true http-addr:127.0.0.1:6784 ipalloc-init:consensus=1 ipalloc-range:10.244.0.0/16 metrics-addr:0.0.0.0:6782 name:ce:18:62:b0:72:58 nickname:node01 no-dns:true no-masq-local:true port:6783]
INFO: 2024/02/29 15:29:32.290436 weave  2.8.1
INFO: 2024/02/29 15:29:33.570890 Bridge type is bridged_fastdp
INFO: 2024/02/29 15:29:33.570925 Communication between peers is unencrypted.
INFO: 2024/02/29 15:29:33.574493 Our name is ce:18:62:b0:72:58(node01)
INFO: 2024/02/29 15:29:33.574550 Launch detected - using supplied peer list: [192.33.135.6]
INFO: 2024/02/29 15:29:33.574588 Using "no-masq-local" LocalRangeTracker
INFO: 2024/02/29 15:29:33.574593 Checking for pre-existing addresses on weave bridge
INFO: 2024/02/29 15:29:33.576271 [allocator ce:18:62:b0:72:58] No valid persisted data
INFO: 2024/02/29 15:29:33.589955 [allocator ce:18:62:b0:72:58] Initialising via deferred consensus
INFO: 2024/02/29 15:29:33.590042 Sniffing traffic on datapath (via ODP)
INFO: 2024/02/29 15:29:33.592137 ->[192.33.135.6:6783] attempting connection
INFO: 2024/02/29 15:29:33.593547 ->[192.33.135.6:6783|32:dc:b5:fe:cc:e5(controlplane)]: connection ready; using protocol version 2
INFO: 2024/02/29 15:29:33.593768 overlay_switch ->[32:dc:b5:fe:cc:e5(controlplane)] using fastdp
INFO: 2024/02/29 15:29:33.593807 ->[192.33.135.6:6783|32:dc:b5:fe:cc:e5(controlplane)]: connection added (new peer)
INFO: 2024/02/29 15:29:33.594127 Listening for HTTP control messages on 127.0.0.1:6784
INFO: 2024/02/29 15:29:33.594162 Listening for metrics requests on 0.0.0.0:6782
INFO: 2024/02/29 15:29:33.696890 ->[192.33.135.6:6783|32:dc:b5:fe:cc:e5(controlplane)]: connection fully established
INFO: 2024/02/29 15:29:33.697826 sleeve ->[192.33.135.6:6783|32:dc:b5:fe:cc:e5(controlplane)]: Effective MTU verified at 1388
INFO: 2024/02/29 15:29:34.249653 [kube-peers] Added myself to peer list &{[{32:dc:b5:fe:cc:e5 controlplane} {ce:18:62:b0:72:58 node01}]}
DEBU: 2024/02/29 15:29:34.255254 [kube-peers] Nodes that have disappeared: map[]
INFO: 2024/02/29 15:29:34.273089 adding entry 10.244.192.0/18 to weaver-no-masq-local of 0
INFO: 2024/02/29 15:29:34.273118 added entry 10.244.192.0/18 to weaver-no-masq-local of 0
10.244.192.0
DEBU: 2024/02/29 15:29:34.368028 registering for updates for node delete events
INFO: 2024/02/29 15:29:38.759600 Discovered remote MAC 3e:de:29:b6:1d:af at 32:dc:b5:fe:cc:e5(controlplane)
INFO: 2024/02/29 15:29:38.760007 Discovered remote MAC a2:67:19:0b:48:5e at 32:dc:b5:fe:cc:e5(controlplane)
INFO: 2024/02/29 15:29:38.804386 Error checking version: Get "https://checkpoint-api.weave.works/v1/check/weave-net?arch=amd64&flag_docker-version=none&flag_kernel-version=5.4.0-1106-gcp&flag_network=fastdp&os=linux&signature=0cAiYxBCWRFmA65t4Mf7qDW6V9VJ3mhvv0Y6rUYbXck%3D&version=2.8.1": dial tcp: lookup checkpoint-api.weave.works on 10.96.0.10:53: no such host

controlplane ~ ➜  k logs -n kube-system weave-net-n2v85 | grep ipalloc-range
Defaulted container "weave" out of: weave, weave-npc, weave-init (init)
INFO: 2024/02/29 15:29:32.290384 Command line options: map[conn-limit:200 datapath:datapath db-prefix:/weavedb/weave-net docker-api: expect-npc:true http-addr:127.0.0.1:6784 ipalloc-init:consensus=1 ipalloc-range:10.244.0.0/16 metrics-addr:0.0.0.0:6782 name:ce:18:62:b0:72:58 nickname:node01 no-dns:true no-masq-local:true port:6783]

controlplane ~ ➜  
```

    ipalloc-range:10.244.0.0/16


### What is the IP Range configured for the services within the cluster?

Inspect the setting on ```kube-api``` server by running on command ```cat /etc/kubernetes/manifests/kube-apiserver.yaml   | grep cluster-ip-range```

```ruby
controlplane ~ ➜  cat /etc/kubernetes/manifests/kube-apiserver.yaml   | grep cluster-ip-range
    - --service-cluster-ip-range=10.96.0.0/12

controlplane ~ ➜  cat /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.33.135.6:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.33.135.6
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    image: registry.k8s.io/kube-apiserver:v1.29.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 192.33.135.6
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-apiserver
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 192.33.135.6
        path: /readyz
        port: 6443
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 250m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 192.33.135.6
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}

controlplane ~ ➜  
```

     - --service-cluster-ip-range=10.96.0.0/12

### How many kube-proxy pods are deployed in this cluster?

Run the command: 

    kubectl get pods -n kube-system 
    
and look for kube-proxy pods.

```ruby
controlplane ~ ➜  kubectl get pods -n kube-system
NAME                                   READY   STATUS    RESTARTS      AGE
coredns-69f9c977-sxwr9                 1/1     Running   0             49m
coredns-69f9c977-zks94                 1/1     Running   0             49m
etcd-controlplane                      1/1     Running   0             49m
kube-apiserver-controlplane            1/1     Running   0             49m
kube-controller-manager-controlplane   1/1     Running   0             49m
kube-proxy-789v8                       1/1     Running   0             48m
kube-proxy-crrqz                       1/1     Running   0             49m
kube-scheduler-controlplane            1/1     Running   0             49m
weave-net-n2v85                        2/2     Running   0             48m
weave-net-t88q9                        2/2     Running   1 (49m ago)   49m

controlplane ~ ➜  kubectl get pods -n kube-system | grep kube-proxy
kube-proxy-789v8                       1/1     Running   0             49m
kube-proxy-crrqz                       1/1     Running   0             49m

controlplane ~ ➜  
```

### What type of proxy is the ```kube-proxy``` configured to use?

```ruby

controlplane ~ ➜  k get pod -n kube-system | grep kube-proxy
kube-proxy-ndb4x                       1/1     Running   0              107m
kube-proxy-z9mj2                       1/1     Running   0              107m

controlplane ~ ➜  kubectl logs -n kube-system kube-proxy-ndb4x 
I0301 05:39:22.257371       1 server_others.go:72] "Using iptables proxy"
I0301 05:39:22.275143       1 server.go:1050] "Successfully retrieved node IP(s)" IPs=["192.9.245.3"]
I0301 05:39:22.298708       1 conntrack.go:58] "Setting nf_conntrack_max" nfConntrackMax=1179648
I0301 05:39:22.299855       1 conntrack.go:118] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_established" value=86400
I0301 05:39:22.344798       1 server.go:652] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0301 05:39:22.344830       1 server_others.go:168] "Using iptables Proxier"
I0301 05:39:22.347328       1 server_others.go:512] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0301 05:39:22.347347       1 server_others.go:529] "Defaulting to no-op detect-local"
I0301 05:39:22.347369       1 proxier.go:246] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0301 05:39:22.365273       1 server.go:865] "Version info" version="v1.29.0"
I0301 05:39:22.365304       1 server.go:867] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0301 05:39:22.365888       1 config.go:188] "Starting service config controller"
I0301 05:39:22.365943       1 shared_informer.go:311] Waiting for caches to sync for service config
I0301 05:39:22.365989       1 config.go:97] "Starting endpoint slice config controller"
I0301 05:39:22.366016       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0301 05:39:22.366099       1 config.go:315] "Starting node config controller"
I0301 05:39:22.366142       1 shared_informer.go:311] Waiting for caches to sync for node config
I0301 05:39:22.466114       1 shared_informer.go:318] Caches are synced for service config
I0301 05:39:22.467066       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0301 05:39:22.467095       1 shared_informer.go:318] Caches are synced for node config

controlplane ~ ➜

controlplane ~ ➜  kubectl logs -n kube-system kube-proxy-ndb4x | grep iptables
I0301 05:39:22.257371       1 server_others.go:72] "Using iptables proxy"
I0301 05:39:22.344830       1 server_others.go:168] "Using iptables Proxier"
I0301 05:39:22.347369       1 proxier.go:246] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"

controlplane ~ ➜  
```


### How does this Kubernetes cluster ensure that a ```kube-proxy``` pod runs on all nodes in the cluster?

Inspect the ```kube-proxy``` pods and try to identify how they are deployed.

    Using DeamonSet

```ruby

controlplane ~ ➜  k get ds -n kube-system 
NAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-proxy   2         2         2       2            2           kubernetes.io/os=linux   113m
weave-net    2         2         2       2            2           <none>                   113m

controlplane ~ ➜


controlplane ~ ➜  kubectl describe pod -n kube-system kube-proxy-ndb4x 
Name:                 kube-proxy-ndb4x
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Service Account:      kube-proxy
Node:                 node01/192.9.245.3
Start Time:           Fri, 01 Mar 2024 05:39:20 +0000
Labels:               controller-revision-hash=7958dbcbd9
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   192.9.245.3
IPs:
  IP:           192.9.245.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://4386aefd07df80cd97059d624166f0ca8281d475f98fdc7246ec9add1357dabc
    Image:         registry.k8s.io/kube-proxy:v1.29.0
    Image ID:      registry.k8s.io/kube-proxy@sha256:8da4de35c4929411300eb8052fdfd34095b6090ed0c8dbc776d58bf1c61a2c89
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 01 Mar 2024 05:39:22 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5plfq (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-5plfq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:                      <none>

controlplane ~ ➜  
```

