### How many Labels exist on node ```node01```?

Run the command ```kubectl describe node node01``` and count the number of ```labels```.

```ruby
controlplane ~ ➜  kubectl get nodes
NAME           STATUS   ROLES           AGE     VERSION
controlplane   Ready    control-plane   5m54s   v1.27.0
node01         Ready    <none>          5m24s   v1.27.0

controlplane ~ ➜  kubectl describe nodes node01 
Name:               node01
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node01
                    kubernetes.io/os=linux
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"4e:45:19:5b:67:69"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 192.43.63.11
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 07 Feb 2024 14:24:16 -0500
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  node01
  AcquireTime:     <unset>
  RenewTime:       Wed, 07 Feb 2024 14:30:03 -0500
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Wed, 07 Feb 2024 14:24:26 -0500   Wed, 07 Feb 2024 14:24:26 -0500   FlannelIsUp                  Flannel is running on this node
  MemoryPressure       False   Wed, 07 Feb 2024 14:29:54 -0500   Wed, 07 Feb 2024 14:24:16 -0500   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Wed, 07 Feb 2024 14:29:54 -0500   Wed, 07 Feb 2024 14:24:16 -0500   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Wed, 07 Feb 2024 14:29:54 -0500   Wed, 07 Feb 2024 14:24:16 -0500   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Wed, 07 Feb 2024 14:29:54 -0500   Wed, 07 Feb 2024 14:24:24 -0500   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.43.63.11
  Hostname:    node01
Capacity:
  cpu:                36
  ephemeral-storage:  1016057248Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             214587056Ki
  pods:               110
Allocatable:
  cpu:                36
  ephemeral-storage:  936398358207
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             214484656Ki
  pods:               110
System Info:
  Machine ID:                 2ec201a93f8a4623aa48bf37d16bd1d6
  System UUID:                1a5d0c79-cc3c-637d-7715-53bdc617d71e
  Boot ID:                    17435083-82a4-462a-b5e2-f9b431583478
  Kernel Version:             5.4.0-1106-gcp
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.6
  Kubelet Version:            v1.27.0
  Kube-Proxy Version:         v1.27.0
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
Non-terminated Pods:          (2 in total)
  Namespace                   Name                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                     ------------  ----------  ---------------  -------------  ---
  kube-flannel                kube-flannel-ds-shhqn    100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      5m56s
  kube-system                 kube-proxy-c6dln         0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m56s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (0%)  100m (0%)
  memory             50Mi (0%)  50Mi (0%)
  ephemeral-storage  0 (0%)     0 (0%)
  hugepages-1Gi      0 (0%)     0 (0%)
  hugepages-2Mi      0 (0%)     0 (0%)
Events:
  Type     Reason                   Age                    From             Message
  ----     ------                   ----                   ----             -------
  Normal   Starting                 5m53s                  kube-proxy       
  Normal   Starting                 5m56s                  kubelet          Starting kubelet.
  Warning  InvalidDiskCapacity      5m56s                  kubelet          invalid capacity 0 on image filesystem
  Normal   NodeHasSufficientMemory  5m56s (x2 over 5m56s)  kubelet          Node node01 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    5m56s (x2 over 5m56s)  kubelet          Node node01 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     5m56s (x2 over 5m56s)  kubelet          Node node01 status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  5m55s                  kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           5m53s                  node-controller  Node node01 event: Registered Node node01 in Controller
  Normal   NodeReady                5m48s                  kubelet          Node node01 status is now: NodeReady

```


### What is the value set to the label key ```beta.kubernetes.io/arch``` on ```node01```?

Run the command: ```kubectl describe node node01``` OR ```kubectl get node node01 --show-labels``` and check the value for the given label key.


```ruby
controlplane ~ ✖ kubectl get node node01 --show-labels
NAME     STATUS   ROLES    AGE     VERSION   LABELS
node01   Ready    <none>   9m50s   v1.27.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node01,kubernetes.io/os=linux

```


### Apply a ```label color=blue``` to node ```node01```

```color = blue```

Run the command: ```kubectl label node node01 color=blue```

```ruby
controlplane ~ ➜  kubectl label node node01 color=blue
node/node01 labeled

controlplane ~ ➜  kubectl get node node01 --show-labels
NAME     STATUS   ROLES    AGE   VERSION   LABELS
node01   Ready    <none>   12m   v1.27.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,color=blue,kubernetes.io/arch=amd64,kubernetes.io/hostname=node01,kubernetes.io/os=linux

```
### Create a new deployment named ```blue``` with the ```nginx``` image and 3 replicas.


    Name: blue
    
    Replicas: 3
    
    Image: nginx

Run the command: ```kubectl create deployment blue --image=nginx --replicas=3```


```ruby
controlplane ~ ➜  kubectl create deployment blue --image=nginx --replicas=3 -o yaml > nginx-deploy.yaml

controlplane ~ ➜  ls
nginx-deploy.yaml  sample.yaml

controlplane ~ ➜  cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: "2024-02-07T19:39:59Z"
  generation: 1
  labels:
    app: blue
  name: blue
  namespace: default
  resourceVersion: "1861"
  uid: 57db2e22-d375-4bcd-a464-99c7f21413d5
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: blue
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: blue
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status: {}

```
### Which nodes ```can``` the pods for the ```blue``` deployment be placed on?

Make sure to check ```taints``` on both nodes!


Check if controlplane and node01 have any taints on them that will prevent the pods to be scheduled on them. If there are no taints, the pods can be scheduled on either node.

So run the following command to check the taints on both nodes.

    kubectl describe node controlplane | grep -i taints
    
    kubectl describe node node01 | grep -i taints

```ruby
controlplane ~ ➜  kubectl get pod
NAME                    READY   STATUS    RESTARTS   AGE
blue-5d9664c795-224j2   1/1     Running   0          4m34s
blue-5d9664c795-nzs6f   1/1     Running   0          4m34s
blue-5d9664c795-zz7s8   1/1     Running   0          4m34s

controlplane ~ ➜  kubectl get pod -o wide
NAME                    READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
blue-5d9664c795-224j2   1/1     Running   0          4m44s   10.244.1.2   node01         <none>           <none>
blue-5d9664c795-nzs6f   1/1     Running   0          4m44s   10.244.0.4   controlplane   <none>           <none>
blue-5d9664c795-zz7s8   1/1     Running   0          4m44s   10.244.1.3   node01         <none>           <none>

```
### Set Node Affinity to the deployment to place the pods on node01 only.

    Name: blue
    
    Replicas: 3
    
    Image: nginx
    
    NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
    
    Key: color
    
    value: blue

Update the deployment by running kubectl edit deployment blue and add the nodeaffinity section as follows:

    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: blue
    spec:
      replicas: 3
      selector:
        matchLabels:
          run: nginx
      template:
        metadata:
          labels:
            run: nginx
        spec:
          containers:
          - image: nginx
            imagePullPolicy: Always
            name: nginx
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: color
                    operator: In
                    values:
                    - blue

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/7491ae43-c301-4f0d-9c13-927bb7a4abd2)

### Which nodes are the pods placed on now?

```ruby
controlplane ~ ➜  kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE    IP           NODE     NOMINATED NODE   READINESS GATES
blue-f69d4c887-9z22n   1/1     Running   0          3m3s   10.244.1.5   node01   <none>           <none>
blue-f69d4c887-df5zm   1/1     Running   0          3m1s   10.244.1.6   node01   <none>           <none>
blue-f69d4c887-wnp59   1/1     Running   0          3m5s   10.244.1.4   node01   <none>           <none>

```


### Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only.


Use the label key - ````node-role.kubernetes.io/control-plane``` - which is already set on the ```controlplane``` node.




    Name: red
    
    Replicas: 2
    
    Image: nginx
    
    NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
    
    Key: node-role.kubernetes.io/control-plane
    
    Use the right operator


Create the file reddeploy.yaml file as follows:


    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: red
    spec:
      replicas: 2
      selector:
        matchLabels:
          run: nginx
      template:
        metadata:
          labels:
            run: nginx
        spec:
          containers:
          - image: nginx
            imagePullPolicy: Always
            name: nginx
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Exists


Then run ```kubectl create -f reddeploy.yaml```

```ruby
controlplane ~ ➜  ls
nginx-deploy.yaml  sample.yaml

controlplane ~ ➜  cat > reddeploy.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
controlplane ~ ➜  

controlplane ~ ➜  kubectl create -f reddeploy.yaml 
deployment.apps/red created

controlplane ~ ➜  kubectl get deployments.apps 
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   3/3     3            3           25m
red    2/2     2            2           8s

controlplane ~ ➜  kubectl describe deployments.apps red 
Name:                   red
Namespace:              default
CreationTimestamp:      Wed, 07 Feb 2024 15:05:00 -0500
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               run=nginx
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  run=nginx
  Containers:
   nginx:
    Image:        nginx
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   red-c89c5dbcb (2/2 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  25s   deployment-controller  Scaled up replica set red-c89c5dbcb to 2

controlplane ~ ➜  kubectl describe deploy red 
Name:                   red
Namespace:              default
CreationTimestamp:      Wed, 07 Feb 2024 15:05:00 -0500
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               run=nginx
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  run=nginx
  Containers:
   nginx:
    Image:        nginx
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   red-c89c5dbcb (2/2 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  45s   deployment-controller  Scaled up replica set red-c89c5dbcb to 2

controlplane ~ ➜  kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
blue-f69d4c887-9z22n   1/1     Running   0          8m23s   10.244.1.5   node01         <none>           <none>
blue-f69d4c887-df5zm   1/1     Running   0          8m21s   10.244.1.6   node01         <none>           <none>
blue-f69d4c887-wnp59   1/1     Running   0          8m25s   10.244.1.4   node01         <none>           <none>
red-c89c5dbcb-mnbkj    1/1     Running   0          2m21s   10.244.0.6   controlplane   <none>           <none>
red-c89c5dbcb-wqgdn    1/1     Running   0          2m21s   10.244.0.5   controlplane   <none>           <none>

```

