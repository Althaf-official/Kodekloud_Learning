### How many static pods exist in this cluster in all namespaces?

Run the command ```kubectl get pods --all-namespaces``` and look for those with ```-controlplane``` appended in the name

### 4

```ruby
controlplane ~ ➜  kubectl get pod --all-namespaces 
NAMESPACE      NAME                                   READY   STATUS    RESTARTS   AGE
kube-flannel   kube-flannel-ds-5n7ml                  1/1     Running   0          17m
kube-flannel   kube-flannel-ds-wbd5w                  1/1     Running   0          17m
kube-system    coredns-5d78c9869d-kmvxx               1/1     Running   0          17m
kube-system    coredns-5d78c9869d-m2944               1/1     Running   0          17m
kube-system    etcd-controlplane                      1/1     Running   0          17m
kube-system    kube-apiserver-controlplane            1/1     Running   0          17m
kube-system    kube-controller-manager-controlplane   1/1     Running   0          17m
kube-system    kube-proxy-8s6cg                       1/1     Running   0          17m
kube-system    kube-proxy-j5ddf                       1/1     Running   0          17m
kube-system    kube-scheduler-controlplane            1/1     Running   0          17m

```
### Which of the below components is NOT deployed as a static pod?

Run ```kubectl get pods --all-namespaces``` and look for the pod from the list that does not end with ```-controlplane```

The ```coredns``` pods are created as part of the ```coredns``` deployment and hence, it is not a static pod.

### Which of the below components is NOT deployed as a static POD?

### On which nodes are the static pods created currently?

By default, ```static``` pods are created for the ```controlplane``` components and hence, they are only created in the ```controlplane``` node.

```ruby
controlplane ~ ✖ kubectl get pods --all-namespaces -o wide 
NAMESPACE      NAME                                   READY   STATUS    RESTARTS   AGE   IP            NODE           NOMINATED NODE   READINESS GATES
kube-flannel   kube-flannel-ds-5n7ml                  1/1     Running   0          24m   192.9.74.3    node01         <none>           <none>
kube-flannel   kube-flannel-ds-wbd5w                  1/1     Running   0          24m   192.9.74.12   controlplane   <none>           <none>
kube-system    coredns-5d78c9869d-kmvxx               1/1     Running   0          24m   10.244.0.2    controlplane   <none>           <none>
kube-system    coredns-5d78c9869d-m2944               1/1     Running   0          24m   10.244.0.3    controlplane   <none>           <none>
kube-system    etcd-controlplane                      1/1     Running   0          25m   192.9.74.12   controlplane   <none>           <none>
kube-system    kube-apiserver-controlplane            1/1     Running   0          25m   192.9.74.12   controlplane   <none>           <none>
kube-system    kube-controller-manager-controlplane   1/1     Running   0          25m   192.9.74.12   controlplane   <none>           <none>
kube-system    kube-proxy-8s6cg                       1/1     Running   0          24m   192.9.74.12   controlplane   <none>           <none>
kube-system    kube-proxy-j5ddf                       1/1     Running   0          24m   192.9.74.3    node01         <none>           <none>
kube-system    kube-scheduler-controlplane            1/1     Running   0          25m   192.9.74.12   controlplane   <none>           <none>

```
### What is the path of the directory holding the static pod definition files?

Run the command ```ps -aux | grep kubelet``` and identify the config file ```- --config=/var/lib/kubelet/config.yaml```. Then check in the config file for staticPodPath.

```ruby
controlplane ~ ➜  ps -aux | grep kubelet
root        3671  0.0  0.1 1107128 321172 ?      Ssl  01:13   1:03 kube-apiserver --advertise-address=192.9.74.12 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
root        4674  0.0  0.0 3849748 92792 ?       Ssl  01:13   0:30 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubele.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.9
root       12677  0.0  0.0   6744   720 pts/1    S+   01:41   0:00 grep --color=auto kubelet

controlplane ~ ➜  cat /var/lib/kubelet/config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerRuntimeEndpoint: ""
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests  <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s


```
### How many pod definition files are present in the manifests directory?

Count the number of files under ```/etc/kubernetes/manifests```

### 4

```ruby
controlplane /etc/kubernetes/manifests ➜  ls
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml

controlplane /etc/kubernetes/manifests ➜  cat etcd.yaml 
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.9.74.12:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.9.74.12:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://192.9.74.12:2380
    - --initial-cluster=controlplane=https://192.9.74.12:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://192.9.74.12:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://192.9.74.12:2380
    - --name=controlplane
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: registry.k8s.io/etcd:3.5.7-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health?exclude=NOSPACE&serializable=true
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health?serializable=false
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
status: {}

controlplane /etc/kubernetes/manifests ➜  cat kube-apiserver.yaml 
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.9.74.12:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.9.74.12
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    image: registry.k8s.io/kube-apiserver:v1.27.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 192.9.74.12
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-apiserver
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 192.9.74.12
        path: /readyz
        port: 6443
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 250m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 192.9.74.12
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}

controlplane /etc/kubernetes/manifests ➜  cat kube-controller-manager.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    - --allocate-node-cidrs=true
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-cidr=10.244.0.0/16
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --use-service-account-credentials=true
    image: registry.k8s.io/kube-controller-manager:v1.27.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-controller-manager
    resources:
      requests:
        cpu: 200m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      name: flexvolume-dir
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /etc/kubernetes/controller-manager.conf
      name: kubeconfig
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      type: DirectoryOrCreate
    name: flexvolume-dir
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /etc/kubernetes/controller-manager.conf
      type: FileOrCreate
    name: kubeconfig
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}

controlplane /etc/kubernetes/manifests ➜  ls
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml

controlplane /etc/kubernetes/manifests ➜  cat kube-scheduler.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --bind-address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=true
    image: registry.k8s.io/kube-scheduler:v1.27.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-scheduler
    resources:
      requests:
        cpu: 100m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig
status: {}

controlplane /etc/kubernetes/manifests ➜  
```

### What is the docker image used to deploy the kube-api server as a static pod?

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/d2fa4ad5-c0c2-416e-975a-2e3d581651e4)

### Create a static pod named static-busybox that uses the busybox image and the command sleep 1000




    
    Name: static-busybox
    
    Image: busybox


Create a pod definition file called ```static-busybox.yaml``` with the provided specs and place it under ```/etc/kubernetes/manifests``` directory.

Create a pod definition file in the manifests folder. To do this, run the command:

    kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml

```ruby
controlplane ~ ➜  kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml

controlplane ~ ➜  cat /etc/kubernetes/manifests/static-busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: static-busybox
  name: static-busybox
spec:
  containers:
  - command:
    - sleep
    - "1000"
    image: busybox
    name: static-busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
controlplane ~ ➜  kubectl get pod --all-namespaces 
NAMESPACE      NAME                                   READY   STATUS    RESTARTS   AGE
default        static-busybox-controlplane            1/1     Running   0          2m23s  <<<<<<<<<<<<<<<<<<<<<<<<<<<<<
kube-flannel   kube-flannel-ds-5n7ml                  1/1     Running   0          41m
kube-flannel   kube-flannel-ds-wbd5w                  1/1     Running   0          41m
kube-system    coredns-5d78c9869d-kmvxx               1/1     Running   0          41m
kube-system    coredns-5d78c9869d-m2944               1/1     Running   0          41m
kube-system    etcd-controlplane                      1/1     Running   0          41m
kube-system    kube-apiserver-controlplane            1/1     Running   0          41m
kube-system    kube-controller-manager-controlplane   1/1     Running   0          41m
kube-system    kube-proxy-8s6cg                       1/1     Running   0          41m
kube-system    kube-proxy-j5ddf                       1/1     Running   0          41m
kube-system    kube-scheduler-controlplane            1/1     Running   0          41m
```

### Edit the image on the static pod to use busybox:1.28.4


    Name: static-busybox
    
    Image: busybox:1.28.4

Simply edit the static pod definition file and save it. If that does not re-create the pod, run: 


    kubectl run --restart=Never --image=busybox:1.28.4 static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml

```ruby
controlplane ~ ➜  kubectl run --restart=Never --image=busybox:1.28.4 static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml

controlplane ~ ➜  kubectl get pod --all-namespaces NAMESPACE      NAME                                   READY   STATUS    RESTARTS   AGE
default        static-busybox-controlplane            1/1     Running   0          6m2s
kube-flannel   kube-flannel-ds-5n7ml                  1/1     Running   0          45m
kube-flannel   kube-flannel-ds-wbd5w                  1/1     Running   0          45m
kube-system    coredns-5d78c9869d-kmvxx               1/1     Running   0          45m
kube-system    coredns-5d78c9869d-m2944               1/1     Running   0          45m
kube-system    etcd-controlplane                      1/1     Running   0          45m
kube-system    kube-apiserver-controlplane            1/1     Running   0          45m
kube-system    kube-controller-manager-controlplane   1/1     Running   0          45m
kube-system    kube-proxy-8s6cg                       1/1     Running   0          45m
kube-system    kube-proxy-j5ddf                       1/1     Running   0          45m
kube-system    kube-scheduler-controlplane            1/1     Running   0          45m

controlplane ~ ➜  ls /etc/kubernetes/manifests/
etcd.yaml            kube-controller-manager.yaml  static-busybox.yaml
kube-apiserver.yaml  kube-scheduler.yaml

controlplane ~ ➜  cat /etc/kubernetes/manifests/static-busybox.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: static-busybox
  name: static-busybox
spec:
  containers:
  - command:
    - sleep
    - "1000"
    image: busybox:1.28.4
    name: static-busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

controlplane ~ ➜  kubectl get pod --all-namespaces 
NAMESPACE      NAME                                   READY   STATUS    RESTARTS   AGE
default        static-busybox-controlplane            1/1     Running   0          109s
kube-flannel   kube-flannel-ds-5n7ml                  1/1     Running   0          47m
kube-flannel   kube-flannel-ds-wbd5w                  1/1     Running   0          47m
kube-system    coredns-5d78c9869d-kmvxx               1/1     Running   0          47m
kube-system    coredns-5d78c9869d-m2944               1/1     Running   0          47m
kube-system    etcd-controlplane                      1/1     Running   0          47m
kube-system    kube-apiserver-controlplane            1/1     Running   0          47m
kube-system    kube-controller-manager-controlplane   1/1     Running   0          47m
kube-system    kube-proxy-8s6cg                       1/1     Running   0          47m
kube-system    kube-proxy-j5ddf                       1/1     Running   0          47m
kube-system    kube-scheduler-controlplane            1/1     Running   0          47m
```

### We just created a new static pod named static-greenbox. Find it and delete it.


This question is a bit tricky. But if you use the knowledge you gained in the previous questions in this lab, you should be able to find the answer to it.


Identify which node the static pod is created on, ssh to the node and delete the pod definition file.
If you don't know the IP of the node, run the ```kubectl get nodes -o wide``` command and identify the IP.
Then, SSH to the node using that IP. For static pod manifest path look at the file ```/var/lib/kubelet/config.yaml``` on node01

First, let's identify the node in which the pod called static-greenbox is created. To do this, run:
root@controlplane:~# kubectl get pods --all-namespaces -o wide  | grep static-greenbox
default       static-greenbox-node01                 1/1     Running   0          19s     10.244.1.2   node01       <none>           <none>
root@controlplane:~#
From the result of this command, we can see that the pod is running on node01.




Next, SSH to node01 and identify the path configured for static pods in this node.

Important: The path need not be /etc/kubernetes/manifests. Make sure to check the path configured in the kubelet configuration file.
    root@controlplane:~# ssh node01 
    root@node01:~# ps -ef |  grep /usr/bin/kubelet 
    root        4147       1  0 14:05 ?        00:00:00 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.9
    root        4773    4733  0 14:05 pts/0    00:00:00 grep /usr/bin/kubelet

    root@node01:~# grep -i staticpod /var/lib/kubelet/config.yaml
    staticPodPath: /etc/just-to-mess-with-you
    
    root@node01:~# 
Here the staticPodPath is /etc/just-to-mess-with-you


Navigate to this directory and delete the YAML file:
    root@node01:/etc/just-to-mess-with-you# ls
    greenbox.yaml
    root@node01:/etc/just-to-mess-with-you# rm -rf greenbox.yaml 
    root@node01:/etc/just-to-mess-with-you#
Exit out of node01 using CTRL + D or type exit. You should return to the controlplane node. Check if the static-greenbox pod has been deleted:
    root@controlplane:~# kubectl get pods --all-namespaces -o wide  | grep static-greenbox
    root@controlplane:~# 



```ruby
controlplane ~ ➜  kubectl get pod --all-namespaces -o wide
NAMESPACE      NAME                                   READY   STATUS    RESTARTS   AGE    IP            NODE           NOMINATED NODE   READINESS GATES
default        static-busybox-controlplane            1/1     Running   0          4m6s   10.244.0.5    controlplane   <none>           <none>
default        static-greenbox-node01                 1/1     Running   0          100s   10.244.1.2    node01         <none>           <none>
kube-flannel   kube-flannel-ds-5n7ml                  1/1     Running   0          49m    192.9.74.3    node01         <none>           <none>
kube-flannel   kube-flannel-ds-wbd5w                  1/1     Running   0          49m    192.9.74.12   controlplane   <none>           <none>
kube-system    coredns-5d78c9869d-kmvxx               1/1     Running   0          49m    10.244.0.2    controlplane   <none>           <none>
kube-system    coredns-5d78c9869d-m2944               1/1     Running   0          49m    10.244.0.3    controlplane   <none>           <none>
kube-system    etcd-controlplane                      1/1     Running   0          50m    192.9.74.12   controlplane   <none>           <none>
kube-system    kube-apiserver-controlplane            1/1     Running   0          50m    192.9.74.12   controlplane   <none>           <none>
kube-system    kube-controller-manager-controlplane   1/1     Running   0          50m    192.9.74.12   controlplane   <none>           <none>
kube-system    kube-proxy-8s6cg                       1/1     Running   0          49m    192.9.74.12   controlplane   <none>           <none>
kube-system    kube-proxy-j5ddf                       1/1     Running   0          49m    192.9.74.3    node01         <none>           <none>
kube-system    kube-scheduler-controlplane            1/1     Running   0          50m    192.9.74.12   controlplane   <none>           <none>

controlplane ~ ➜  ssh node01

root@node01 ~ ➜  ls

root@node01 ~ ➜  cd /etc/kubernetes/manifests/

root@node01 /etc/kubernetes/manifests ➜  ls

root@node01 /etc/kubernetes/manifests ➜  cat /var/lib/kubelet/config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerRuntimeEndpoint: ""
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/just-to-mess-with-you
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s

root@node01 /etc/kubernetes/manifests ➜  cat  /etc/just-to-mess-with-you
cat: /etc/just-to-mess-with-you: Is a directory

root@node01 /etc/kubernetes/manifests ✖ cd  /etc/just-to-mess-with-you

root@node01 /etc/just-to-mess-with-you ➜  ls
greenbox.yaml

root@node01 /etc/just-to-mess-with-you ➜  cat greenbox.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: static-greenbox
  name: static-greenbox
spec:
  containers:
  - command:
    - sleep
    - "3000"
    image: busybox
    name: static-greenbox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

root@node01 /etc/just-to-mess-with-you ➜  cat > greenbox.yaml 

root@node01 /etc/just-to-mess-with-you ➜  ls
greenbox.yaml

root@node01 /etc/just-to-mess-with-you ➜  kubectl get pod --all-namespaces
E0209 02:08:22.228754   10729 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0209 02:08:22.229919   10729 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0209 02:08:22.230486   10729 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0209 02:08:22.232145   10729 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0209 02:08:22.233684   10729 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
The connection to the server localhost:8080 was refused - did you specify the right host or port?

root@node01 /etc/just-to-mess-with-you ✖ 
```
