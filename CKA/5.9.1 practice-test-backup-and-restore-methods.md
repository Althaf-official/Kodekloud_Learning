```ruby
controlplane ~ ➜  kubectl get all
NAME                        READY   STATUS    RESTARTS   AGE
pod/blue-6b478c8dbf-fx48m   1/1     Running   0          2m32s
pod/blue-6b478c8dbf-kxwxw   1/1     Running   0          2m32s
pod/blue-6b478c8dbf-xhj9w   1/1     Running   0          2m32s
pod/red-6684f7669d-g6s8l    1/1     Running   0          2m32s
pod/red-6684f7669d-hnpkw    1/1     Running   0          2m32s

NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/blue-service   NodePort    10.100.85.234   <none>        80:30082/TCP   2m33s
service/kubernetes     ClusterIP   10.96.0.1       <none>        443/TCP        8m4s
service/red-service    NodePort    10.99.123.87    <none>        80:30080/TCP   2m33s

NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/blue   3/3     3            3           2m32s
deployment.apps/red    2/2     2            2           2m32s

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/blue-6b478c8dbf   3         3         3       2m32s
replicaset.apps/red-6684f7669d    2         2         2       2m32s

```

#### We have a working Kubernetes cluster with a set of web applications running. Let us first explore the setup.

How many deployments exist in the cluster in default namespace?

    2

#### What is the version of ETCD running on the cluster?

    Check the ETCD Pod or Process


    Look at the ETCD Logs OR check the image used by ETCD pod.


 Look at the ETCD Logs using the command 
    
    kubectl logs etcd-controlplane -n kube-system
    
or check the image used by the ETCD pod: 
    
    kubectl describe pod etcd-controlplane  -n kube-system
    
```ruby
controlplane ~ ➜  kubectl logs etcd-controlplane -n kube-system | grep -i 'etcd-version'
{"level":"info","ts":"2024-02-15T10:39:36.518Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"amd64","max-cpu-set":36,"max-cpu-available":36,"member-initialized":false,"name":"controlplane","data-dir":"/var/lib/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.20.223.6:2380"],"listen-peer-urls":["https://192.20.223.6:2380"],"advertise-client-urls":["https://192.20.223.6:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.20.223.6:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"controlplane=https://192.20.223.6:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}

controlplane ~ ➜

controlplane ~ ✖ kubectl describe pod etcd-controlplane -n kube-system | grep Image
    Image:         registry.k8s.io/etcd:3.5.7-0
    Image ID:      registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83

controlplane ~ ➜  

```
![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/32c8730e-9353-4c87-8f54-04b145cc7795)
![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/bfb426ce-0776-43f4-99c9-fd579ecacd8c)


#### At what address can you reach the ETCD cluster from the controlplane node?

Check the ETCD Service configuration in the ETCD POD

Use the command 
    
    kubectl describe pod etcd-controlplane -n kube-system 
    
and look for ```--listen-client-urls```


```ruby
controlplane ~ ✖ kubectl describe pod etcd-controlplane -n kube-system | grep '\--listen-c'
    --listen-client-urls=https://127.0.0.1:2379,https://192.20.223.6:2379
controlplane ~ ➜  kubectl describe pod etcd-controlplane -n kube-system Name:                 etcd-controlplane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 controlplane/192.20.223.6
Start Time:           Thu, 15 Feb 2024 05:39:47 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.20.223.6:2379
                      kubernetes.io/config.hash: de2c2892ff8ea5b94542ea42cce538b2
                      kubernetes.io/config.mirror: de2c2892ff8ea5b94542ea42cce538b2
                      kubernetes.io/config.seen: 2024-02-15T05:39:46.220810355-05:00
                      kubernetes.io/config.source: file
Status:               Running
SeccompProfile:       RuntimeDefault
IP:                   192.20.223.6
IPs:
  IP:           192.20.223.6
Controlled By:  Node/controlplane
Containers:
  etcd:
    Container ID:  containerd://372499b3e5528c15b9a2f229f35c58d10d331ac2f456e8ee8cf87df1b7de79ae
    Image:         registry.k8s.io/etcd:3.5.7-0
    Image ID:      registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://192.20.223.6:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --experimental-watch-progress-notify-interval=5s
      --initial-advertise-peer-urls=https://192.20.223.6:2380
      --initial-cluster=controlplane=https://192.20.223.6:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://192.20.223.6:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://192.20.223.6:2380
      --name=controlplane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Thu, 15 Feb 2024 05:39:36 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health%3Fexclude=NOSPACE&serializable=true delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health%3Fserializable=false delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   22m   kubelet  Container image "registry.k8s.io/etcd:3.5.7-0" already present on machine
  Normal  Created  22m   kubelet  Created container etcd
  Normal  Started  22m   kubelet  Started container etcd

controlplane ~ ➜  
```
    This means that ETCD is reachable on localhost (127.0.0.1) at port 2379.

### Where is the ETCD server certificate file located?

Note this path down as you will need to use it later



    Check the ETCD pod configuration with the command: kubectl describe pod etcd-controlplane  -n kube-system and look for the value for --cert-file:

    root@controlplane:~# kubectl -n kube-system describe pod etcd-controlplane | grep '\--cert-file'
          --cert-file=/etc/kubernetes/pki/etcd/server.crt
    root@controlplane:~#

```ruby
controlplane ~ ➜  kubectl describe pod -n kube-system etcd-controlplane 
Name:                 etcd-controlplane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 controlplane/192.20.223.6
Start Time:           Thu, 15 Feb 2024 05:39:47 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.20.223.6:2379
                      kubernetes.io/config.hash: de2c2892ff8ea5b94542ea42cce538b2
                      kubernetes.io/config.mirror: de2c2892ff8ea5b94542ea42cce538b2
                      kubernetes.io/config.seen: 2024-02-15T05:39:46.220810355-05:00
                      kubernetes.io/config.source: file
Status:               Running
SeccompProfile:       RuntimeDefault
IP:                   192.20.223.6
IPs:
  IP:           192.20.223.6
Controlled By:  Node/controlplane
Containers:
  etcd:
    Container ID:  containerd://372499b3e5528c15b9a2f229f35c58d10d331ac2f456e8ee8cf87df1b7de79ae
    Image:         registry.k8s.io/etcd:3.5.7-0
    Image ID:      registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://192.20.223.6:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --experimental-watch-progress-notify-interval=5s
      --initial-advertise-peer-urls=https://192.20.223.6:2380
      --initial-cluster=controlplane=https://192.20.223.6:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://192.20.223.6:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://192.20.223.6:2380
      --name=controlplane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Thu, 15 Feb 2024 05:39:36 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health%3Fexclude=NOSPACE&serializable=true delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health%3Fserializable=false delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   24m   kubelet  Container image "registry.k8s.io/etcd:3.5.7-0" already present on machine
  Normal  Created  24m   kubelet  Created container etcd
  Normal  Started  24m   kubelet  Started container etcd

controlplane ~ ➜  
```

### Where is the ETCD CA Certificate file located?

Note this path down as you will need to use it later.

    Check the ETCD pod configuration with the command: kubectl describe pod etcd-controlplane  -n kube-system and look for the value of --trusted-ca-file:

    root@controlplane:~# kubectl -n kube-system describe pod etcd-controlplane | grep '\--trusted-ca-file'
          --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    root@controlplane:~#

```ruby
controlplane ~ ➜  kubectl describe pod -n kube-system etcd-controlplane 
Name:                 etcd-controlplane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 controlplane/192.20.223.6
Start Time:           Thu, 15 Feb 2024 05:39:47 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.20.223.6:2379
                      kubernetes.io/config.hash: de2c2892ff8ea5b94542ea42cce538b2
                      kubernetes.io/config.mirror: de2c2892ff8ea5b94542ea42cce538b2
                      kubernetes.io/config.seen: 2024-02-15T05:39:46.220810355-05:00
                      kubernetes.io/config.source: file
Status:               Running
SeccompProfile:       RuntimeDefault
IP:                   192.20.223.6
IPs:
  IP:           192.20.223.6
Controlled By:  Node/controlplane
Containers:
  etcd:
    Container ID:  containerd://372499b3e5528c15b9a2f229f35c58d10d331ac2f456e8ee8cf87df1b7de79ae
    Image:         registry.k8s.io/etcd:3.5.7-0
    Image ID:      registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://192.20.223.6:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --experimental-watch-progress-notify-interval=5s
      --initial-advertise-peer-urls=https://192.20.223.6:2380
      --initial-cluster=controlplane=https://192.20.223.6:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://192.20.223.6:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://192.20.223.6:2380
      --name=controlplane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Thu, 15 Feb 2024 05:39:36 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health%3Fexclude=NOSPACE&serializable=true delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health%3Fserializable=false delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   24m   kubelet  Container image "registry.k8s.io/etcd:3.5.7-0" already present on machine
  Normal  Created  24m   kubelet  Created container etcd
  Normal  Started  24m   kubelet  Started container etcd

controlplane ~ ➜  
```

#### The master node in our cluster is planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.


Store the backup file at location ```/opt/snapshot-pre-boot.db```




    Backup ETCD to /opt/snapshot-pre-boot.db

    
    Use the etcdctl snapshot save command. You will have to make use of additional flags to connect to the ETCD server.
    
    --endpoints: Optional Flag, points to the address where ETCD is running (127.0.0.1:2379)
    
    --cacert: Mandatory Flag (Absolute Path to the CA certificate file)
    
    --cert: Mandatory Flag (Absolute Path to the Server certificate file)
    
    --key: Mandatory Flag (Absolute Path to the Key file)

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/246a655c-1c5a-4392-a1d2-ad5b71199569)

```ruby

controlplane ~ ➜  ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
> --cacert=/etc/kubernetes/pki/etcd/ca.crt \
> --cert=/etc/kubernetes/pki/etcd/server.crt \
> --key=/etc/kubernetes/pki/etcd/server.key \
> snapshot save /opt/snapshot-pre-boot.db
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db

Snapshot saved at /opt/snapshot-pre-boot.db

```

You may also refer the solution here

https://github.com/mmumshad/kubernetes-the-hard-way/blob/master/practice-questions-answers/cluster-maintenance/backup-etcd/etcd-backup-and-restore.md


------

Great! Let us now wait for the maintenance window to finish. Go get some sleep. (Don't go for real)

Click Ok to Continue

### Wake up! We have a conference call! After the reboot the master nodes came back online, but none of our applications are accessible. Check the status of the applications on the cluster. What's wrong?

```ruby
controlplane ~ ➜  kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   36s

controlplane ~ ➜  
```

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/f9291891-fe1d-42fc-b360-8fe872d2aa25)

#### Luckily we took a backup. Restore the original state of the cluster using the backup file.





    Deployments: 2
    
    Services: 3



##### Restore the etcd to a new directory from the snapshot by using the ```etcdctl snapshot restore``` command. Once the directory is restored, update the ETCD configuration to use the restored directory.


First Restore the snapshot:

    root@controlplane:~# ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
    snapshot restore /opt/snapshot-pre-boot.db
    
    
    2022-03-25 09:19:27.175043 I | mvcc: restore compact to 2552
    2022-03-25 09:19:27.266709 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32
    root@controlplane:~# 


Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, the only required option for the restore command is the --data-dir.



Next, update the /etc/kubernetes/manifests/etcd.yaml:

We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, so, the only change to be made in the YAML file, is to change the hostPath for the volume called etcd-data from old directory (/var/lib/etcd) to the new directory (/var/lib/etcd-from-backup).

      volumes:
      - hostPath:
          path: /var/lib/etcd-from-backup
          type: DirectoryOrCreate
        name: etcd-data

        
With this change, /var/lib/etcd on the container points to /var/lib/etcd-from-backup on the controlplane (which is what we want).

When this file is updated, the ETCD pod is automatically re-created as this is a static pod placed under the /etc/kubernetes/manifests directory.



Note 1: As the ETCD pod has changed it will automatically restart, and also kube-controller-manager and kube-scheduler. Wait 1-2 to mins for this pods to restart. You can run the command: watch "crictl ps | grep etcd" to see when the ETCD pod is restarted.

Note 2: If the etcd pod is not getting Ready 1/1, then restart it by kubectl delete pod -n kube-system etcd-controlplane and wait 1 minute.

Note 3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated. You don't have to change anything else.



If you do change --data-dir to /var/lib/etcd-from-backup in the ETCD YAML file, make sure that the volumeMounts for etcd-data is updated as well, with the mountPath pointing to /var/lib/etcd-from-backup (THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)


----------
### failed attempt

```ruby
controlplane ~ ➜  ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
> snapshot restore /opt/snapshot-pre-boot.db
2024-02-15 06:20:36.802654 I | mvcc: restore compact to 2475
2024-02-15 06:20:36.820006 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32

controlplane ~ ➜  kubectl get allNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   5m2s

controlplane ~ ➜  cat /etc/kubernetes/manifests/etcd.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.20.223.6:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.20.223.6:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://192.20.223.6:2380
    - --initial-cluster=controlplane=https://192.20.223.6:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://192.20.223.6:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://192.20.223.6:2380
    - --name=controlplane
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: registry.k8s.io/etcd:3.5.7-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health?exclude=NOSPACE&serializable=true
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health?serializable=false
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
status: {}

controlplane ~ ➜  vi /etc/kubernetes/manifests/etcd.yaml

controlplane ~ ➜  kubectl get all
^C

controlplane ~ ✖ watch "crictl ps | grep etcd"

controlplane ~ ➜  vi /etc/kubernetes/manifests/etcd.yaml

controlplane ~ ➜  watch "crictl ps | grep etcd"

controlplane ~ ➜  kubectl get all
^C^

controlplane ~ ✖ kubectl get all -n kube-system 
No resources found in kube-system namespace.

controlplane ~ ➜  cat /etc/kubernetes/manifests/etcd.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.20.223.6:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.20.223.6:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://192.20.223.6:2380
    - --initial-cluster=controlplane=https://192.20.223.6:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://192.20.223.6:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://192.20.223.6:2380
    - --name=controlplane
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: registry.k8s.io/etcd:3.5.7-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health?exclude=NOSPACE&serializable=true
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health?serializable=false
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd-from-backup
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
status: {}

controlplane ~ ➜  kubectl get all -n kube-system 
No resources found in kube-system namespace.
```
