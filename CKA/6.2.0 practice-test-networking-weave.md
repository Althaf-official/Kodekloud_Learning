### Don't give up. Normally it is the last key on the ring which opens the door.

– Paulo Coelho

----

### How many Nodes are part of this cluster?

Including master and worker nodes

```ruby

controlplane ~ ➜  k get nodes
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   31m   v1.29.0
node01         Ready    <none>          30m   v1.29.0

controlplane ~ ➜

```
### What is the Networking Solution used by this cluster?

Check the config file located at ```/etc/cni/net.d/```

```ruby
controlplane /etc/cni/net.d ➜  ls
10-weave.conflist

controlplane /etc/cni/net.d ➜  cat 10-weave.conflist 
{
    "cniVersion": "0.3.0",
    "name": "weave",
    "plugins": [
        {
            "name": "weave",
            "type": "weave-net",
            "hairpinMode": true
        },
        {
            "type": "portmap",
            "capabilities": {"portMappings": true},
            "snat": true
        }
    ]
}

controlplane /etc/cni/net.d ➜

```

    weave

### How many weave agents/peers are deployed in this cluster?

```ruby
controlplane /etc ➜  k get pods -n kube-system
NAME                                   READY   STATUS    RESTARTS      AGE
coredns-69f9c977-j4wmp                 1/1     Running   0             35m
coredns-69f9c977-nthgp                 1/1     Running   0             35m
etcd-controlplane                      1/1     Running   0             35m
kube-apiserver-controlplane            1/1     Running   0             35m
kube-controller-manager-controlplane   1/1     Running   0             35m
kube-proxy-247gz                       1/1     Running   0             35m
kube-proxy-mlswk                       1/1     Running   0             35m
kube-scheduler-controlplane            1/1     Running   0             35m
weave-net-mrhsw                        2/2     Running   1 (35m ago)   35m
weave-net-vjvhp                        2/2     Running   0             35m

controlplane /etc ➜

controlplane /etc ➜  k get pods -n kube-system
NAME                                   READY   STATUS    RESTARTS      AGE
coredns-69f9c977-j4wmp                 1/1     Running   0             35m
coredns-69f9c977-nthgp                 1/1     Running   0             35m
etcd-controlplane                      1/1     Running   0             35m
kube-apiserver-controlplane            1/1     Running   0             35m
kube-controller-manager-controlplane   1/1     Running   0             35m
kube-proxy-247gz                       1/1     Running   0             35m
kube-proxy-mlswk                       1/1     Running   0             35m
kube-scheduler-controlplane            1/1     Running   0             35m
weave-net-mrhsw                        2/2     Running   1 (35m ago)   35m
weave-net-vjvhp                        2/2     Running   0             35m

controlplane /etc ➜  k get pods -n kube-system | grep weave-net
weave-net-mrhsw                        2/2     Running   1 (36m ago)   36m
weave-net-vjvhp                        2/2     Running   0             36m

controlplane /etc ➜  k describe pods -n kube-system weave-net-mrhsw
Name:                 weave-net-mrhsw
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Service Account:      weave-net
Node:                 controlplane/192.13.7.3
Start Time:           Thu, 29 Feb 2024 07:14:00 +0000
Labels:               controller-revision-hash=58fc49b856
                      name=weave-net
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   192.13.7.3
IPs:
  IP:           192.13.7.3
Controlled By:  DaemonSet/weave-net
Init Containers:
  weave-init:
    Container ID:  containerd://5873d86cf599a5669962b3004cc207e6457d13868f7c3cfac4f0217a1c2956f6
    Image:         docker.io/weaveworks/weave-kube:2.8.1
    Image ID:      docker.io/weaveworks/weave-kube@sha256:d797338e7beb17222e10757b71400d8471bdbd9be13b5da38ce2ebf597fb4e63
    Port:          <none>
    Host Port:     <none>
    Command:
      /home/weave/init.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 29 Feb 2024 07:14:01 +0000
      Finished:     Thu, 29 Feb 2024 07:14:03 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /host/etc from cni-conf (rw)
      /host/home from cni-bin2 (rw)
      /host/opt from cni-bin (rw)
      /lib/modules from lib-modules (rw)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vwfks (ro)
Containers:
  weave:
    Container ID:  containerd://4968f2f0af28ab34816d9414f566fb2c2a9f4b132f321a081895dda96c02c78c
    Image:         docker.io/weaveworks/weave-kube:2.8.1
    Image ID:      docker.io/weaveworks/weave-kube@sha256:d797338e7beb17222e10757b71400d8471bdbd9be13b5da38ce2ebf597fb4e63
    Port:          <none>
    Host Port:     <none>
    Command:
      /home/weave/launch.sh
    State:          Running
      Started:      Thu, 29 Feb 2024 07:14:05 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Thu, 29 Feb 2024 07:14:03 +0000
      Finished:     Thu, 29 Feb 2024 07:14:04 +0000
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:      50m
      memory:   100Mi
    Readiness:  http-get http://127.0.0.1:6784/status delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      HOSTNAME:         (v1:spec.nodeName)
      IPALLOC_RANGE:   10.244.0.0/16
      INIT_CONTAINER:  true
    Mounts:
      /host/etc/machine-id from machine-id (ro)
      /host/var/lib/dbus from dbus (rw)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vwfks (ro)
      /weavedb from weavedb (rw)
  weave-npc:
    Container ID:   containerd://41ab049221566967ccbd84dc3dfc5a6a4cc9ae10f6c2d36f40542152f24be5e2
    Image:          docker.io/weaveworks/weave-npc:2.8.1
    Image ID:       docker.io/weaveworks/weave-npc@sha256:38d3e30a97a2260558f8deb0fc4c079442f7347f27c86660dbfc8ca91674f14c
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Thu, 29 Feb 2024 07:14:04 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     50m
      memory:  100Mi
    Environment:
      HOSTNAME:   (v1:spec.nodeName)
    Mounts:
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vwfks (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  weavedb:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/weave
    HostPathType:  
  cni-bin:
    Type:          HostPath (bare host directory volume)
    Path:          /opt
    HostPathType:  
  cni-bin2:
    Type:          HostPath (bare host directory volume)
    Path:          /home
    HostPathType:  
  cni-conf:
    Type:          HostPath (bare host directory volume)
    Path:          /etc
    HostPathType:  
  dbus:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/dbus
    HostPathType:  
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  machine-id:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/machine-id
    HostPathType:  FileOrCreate
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  kube-api-access-vwfks:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 :NoSchedule op=Exists
                             :NoExecute op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  37m                default-scheduler  Successfully assigned kube-system/weave-net-mrhsw to controlplane
  Normal   Pulled     37m                kubelet            Container image "docker.io/weaveworks/weave-kube:2.8.1" already present on machine
  Normal   Created    37m                kubelet            Created container weave-init
  Normal   Started    37m                kubelet            Started container weave-init
  Normal   Pulled     37m                kubelet            Container image "docker.io/weaveworks/weave-npc:2.8.1" already present on machine
  Normal   Pulled     36m (x2 over 37m)  kubelet            Container image "docker.io/weaveworks/weave-kube:2.8.1" already present on machine
  Normal   Created    36m (x2 over 37m)  kubelet            Created container weave
  Normal   Created    36m                kubelet            Created container weave-npc
  Normal   Started    36m                kubelet            Started container weave-npc
  Normal   Started    36m (x2 over 37m)  kubelet            Started container weave
  Warning  Unhealthy  36m (x2 over 36m)  kubelet            Readiness probe failed: Get "http://127.0.0.1:6784/status": dial tcp 127.0.0.1:6784: connect: connection refused

controlplane /etc ➜  
```
### On which nodes are the weave peers present?

It is present on every node. You can check by running the following command: -

    kubectl get po -owide -n kube-system | grep weave

```ruby
controlplane ~ ➜  kubectl get po -owide -n kube-system | grep weave
weave-net-mrhsw                        2/2     Running   1 (39m ago)   39m   192.13.7.3   controlplane   <none>           <none>
weave-net-vjvhp                        2/2     Running   0             39m   192.13.7.6   node01         <none>           <none>

controlplane ~ ➜   
```

    one on every node

### Identify the name of the bridge network/interface created by weave on each node.

Run the command 

    ip link

```ruby
controlplane ~ ➜  ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: datapath: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/ether 5e:83:cf:07:05:b9 brd ff:ff:ff:ff:ff:ff
4: weave: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue state UP mode DEFAULT group default qlen 1000
    link/ether 12:e3:00:72:f2:1c brd ff:ff:ff:ff:ff:ff
6: vethwe-datapath@vethwe-bridge: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master datapath state UP mode DEFAULT group default 
    link/ether d6:16:48:df:0b:68 brd ff:ff:ff:ff:ff:ff
7: vethwe-bridge@vethwe-datapath: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master weave state UP mode DEFAULT group default 
    link/ether 6e:aa:b3:b8:27:c0 brd ff:ff:ff:ff:ff:ff
8: vxlan-6784: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 65535 qdisc noqueue master datapath state UNKNOWN mode DEFAULT group default qlen 1000
    link/ether 86:e3:20:5d:65:6d brd ff:ff:ff:ff:ff:ff
10: vethwepl4a16434@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master weave state UP mode DEFAULT group default 
    link/ether 16:80:bf:46:b1:ba brd ff:ff:ff:ff:ff:ff link-netns cni-6aef490c-55b7-c69a-2f3b-8b7f648651d0
12: vethweplbcc63ca@if11: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master weave state UP mode DEFAULT group default 
    link/ether f6:ce:fe:2f:f2:82 brd ff:ff:ff:ff:ff:ff link-netns cni-d63fc316-b2b5-2636-260a-d52468fe2a75
2923: eth0@if2924: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT group default 
    link/ether 02:42:c0:0d:07:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0
2925: eth1@if2926: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default 
    link/ether 02:42:ac:19:00:6e brd ff:ff:ff:ff:ff:ff link-netnsid 1

controlplane ~ ➜
controlplane ~ ➜  ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: datapath: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/ether 5e:83:cf:07:05:b9 brd ff:ff:ff:ff:ff:ff
4: weave: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue state UP mode DEFAULT group default qlen 1000
    link/ether 12:e3:00:72:f2:1c brd ff:ff:ff:ff:ff:ff
6: vethwe-datapath@vethwe-bridge: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master datapath state UP mode DEFAULT group default 
    link/ether d6:16:48:df:0b:68 brd ff:ff:ff:ff:ff:ff
7: vethwe-bridge@vethwe-datapath: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master weave state UP mode DEFAULT group default 
    link/ether 6e:aa:b3:b8:27:c0 brd ff:ff:ff:ff:ff:ff
8: vxlan-6784: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 65535 qdisc noqueue master datapath state UNKNOWN mode DEFAULT group default qlen 1000
    link/ether 86:e3:20:5d:65:6d brd ff:ff:ff:ff:ff:ff
10: vethwepl4a16434@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master weave state UP mode DEFAULT group default 
    link/ether 16:80:bf:46:b1:ba brd ff:ff:ff:ff:ff:ff link-netns cni-6aef490c-55b7-c69a-2f3b-8b7f648651d0
12: vethweplbcc63ca@if11: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master weave state UP mode DEFAULT group default 
    link/ether f6:ce:fe:2f:f2:82 brd ff:ff:ff:ff:ff:ff link-netns cni-d63fc316-b2b5-2636-260a-d52468fe2a75
2923: eth0@if2924: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT group default 
    link/ether 02:42:c0:0d:07:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0
2925: eth1@if2926: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default 
    link/ether 02:42:ac:19:00:6e brd ff:ff:ff:ff:ff:ff link-netnsid 1

controlplane ~ ➜  

controlplane ~ ➜  ip link | grep weave
4: weave: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue state UP mode DEFAULT group default qlen 1000
7: vethwe-bridge@vethwe-datapath: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master weave state UP mode DEFAULT group default 
10: vethwepl4a16434@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master weave state UP mode DEFAULT group default 
12: vethweplbcc63ca@if11: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue master weave state UP mode DEFAULT group default 

controlplane ~ ➜ 
```
![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/82b8732d-bbf1-4044-bd5b-6c29aa7187cf)


    weave

### What is the POD IP address range configured by ```weave```?

Run the command 

    ip addr show weave

```ruby

controlplane ~ ➜  ip addr show weave
4: weave: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue state UP group default qlen 1000
    link/ether 12:e3:00:72:f2:1c brd ff:ff:ff:ff:ff:ff
    inet 10.244.0.1/16 brd 10.244.255.255 scope global weave
       valid_lft forever preferred_lft forever

controlplane ~ ➜  
```

    10.xxx.xx.x

### What is the default gateway configured on the PODs scheduled on ```node01```?

Try scheduling a pod on ```node01``` and check ```ip route``` output


SSH to the node01 by running the command: ```ssh node01``` and then run the ```ip route``` command and look at the ```weave``` line.


```ruby
controlplane ~ ➜  ssh node01

node01 ~ ➜  ip route
default via 172.25.0.1 dev eth1 
10.244.0.0/16 dev weave proto kernel scope link src 10.244.192.0 
172.25.0.0/24 dev eth1 proto kernel scope link src 172.25.0.90 
192.13.7.0/24 dev eth0 proto kernel scope link src 192.13.7.6 

node01 ~ ➜  ip route | grep weave
10.244.0.0/16 dev weave proto kernel scope link src 10.244.192.0 

node01 ~ ➜  

```

