#### Let us explore the environment first. How many nodes do you see in the cluster?

Including the controlplane and worker nodes.

```ruby
controlplane ~ ✖ kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   11m   v1.27.0
node01         Ready    <none>          10m   v1.27.0
```

#### How many applications do you see hosted on the cluster?

Check the number of deployments in the default namespace.

```ruby
controlplane ~ ➜  kubectl get deployments
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   3/3     3            3           3m1s
```
### Which nodes are the applications hosted on?

```ruby
controlplane ~ ➜  kubectl get pod -o wide 
NAME                    READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
blue-6b478c8dbf-4h4lc   1/1     Running   0          4m24s   10.244.1.3   node01         <none>           <none>
blue-6b478c8dbf-7f9mx   1/1     Running   0          4m24s   10.244.0.4   controlplane   <none>           <none>
blue-6b478c8dbf-tfzvh   1/1     Running   0          4m24s   10.244.1.2   node01         <none>           <none>

controlplane ~ ➜  
```
### We need to take ```node01``` out for maintenance. Empty the node of all applications and mark it unschedulable.





    Node node01 Unschedulable
    
    Pods evicted from node01


Run the command 

    kubectl drain node01 --ignore-daemonsets


```ruby
controlplane ~ ➜  kubectl get pod -o wide 
NAME                    READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
blue-6b478c8dbf-4h4lc   1/1     Running   0          4m24s   10.244.1.3   node01         <none>           <none>
blue-6b478c8dbf-7f9mx   1/1     Running   0          4m24s   10.244.0.4   controlplane   <none>           <none>
blue-6b478c8dbf-tfzvh   1/1     Running   0          4m24s   10.244.1.2   node01         <none>           <none>

controlplane ~ ➜  kubectl drain node01 --ignore-daemonsets 
node/node01 cordoned
Warning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-ln4m4, kube-system/kube-proxy-544bd
evicting pod default/blue-6b478c8dbf-tfzvh
evicting pod default/blue-6b478c8dbf-4h4lc
pod/blue-6b478c8dbf-4h4lc evicted
pod/blue-6b478c8dbf-tfzvh evicted
node/node01 drained

controlplane ~ ➜  kubectl get pod -o wide 
NAME                    READY   STATUS    RESTARTS   AGE    IP           NODE           NOMINATED NODE   READINESS GATES
blue-6b478c8dbf-7f9mx   1/1     Running   0          7m4s   10.244.0.4   controlplane   <none>           <none>
blue-6b478c8dbf-l77h6   1/1     Running   0          38s    10.244.0.6   controlplane   <none>           <none>
blue-6b478c8dbf-rqkpn   1/1     Running   0          38s    10.244.0.5   controlplane   <none>           <none>

controlplane ~ ➜  kubectl get node -o wide 
NAME           STATUS                     ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
controlplane   Ready                      control-plane   19m   v1.27.0   192.8.180.10   <none>        Ubuntu 20.04.6 LTS   5.4.0-1106-gcp   containerd://1.6.6
node01         Ready,SchedulingDisabled   <none>          18m   v1.27.0   192.8.180.3    <none>        Ubuntu 20.04.5 LTS   5.4.0-1106-gcp   containerd://1.6.6

controlplane ~ ➜  
```

### What nodes are the apps on now?

```ruby
controlplane ~ ➜  kubectl get pod -o wide 
NAME                    READY   STATUS    RESTARTS   AGE    IP           NODE           NOMINATED NODE   READINESS GATES
blue-6b478c8dbf-7f9mx   1/1     Running   0          7m4s   10.244.0.4   controlplane   <none>           <none>
blue-6b478c8dbf-l77h6   1/1     Running   0          38s    10.244.0.6   controlplane   <none>           <none>
blue-6b478c8dbf-rqkpn   1/1     Running   0          38s    10.244.0.5   controlplane   <none>           <none>

```
### The maintenance tasks have been completed. Configure the node node01 to be schedulable again.

Run the command 

      kubectl uncordon node01


```ruby
controlplane ~ ➜  kubectl get node -o wide 
NAME           STATUS                     ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
controlplane   Ready                      control-plane   19m   v1.27.0   192.8.180.10   <none>        Ubuntu 20.04.6 LTS   5.4.0-1106-gcp   containerd://1.6.6
node01         Ready,SchedulingDisabled   <none>          18m   v1.27.0   192.8.180.3    <none>        Ubuntu 20.04.5 LTS   5.4.0-1106-gcp   containerd://1.6.6

controlplane ~ ➜  kubectl uncordon node01 
node/node01 uncordoned

controlplane ~ ➜  kubectl get node -o wide 
NAME           STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
controlplane   Ready    control-plane   22m   v1.27.0   192.8.180.10   <none>        Ubuntu 20.04.6 LTS   5.4.0-1106-gcp   containerd://1.6.6
node01         Ready    <none>          21m   v1.27.0   192.8.180.3    <none>        Ubuntu 20.04.5 LTS   5.4.0-1106-gcp   containerd://1.6.6

controlplane ~ ➜  kubectl get pod -o wide 
NAME                    READY   STATUS    RESTARTS   AGE    IP           NODE           NOMINATED NODE   READINESS GATES
blue-6b478c8dbf-7f9mx   1/1     Running   0          10m    10.244.0.4   controlplane   <none>           <none>
blue-6b478c8dbf-l77h6   1/1     Running   0          4m1s   10.244.0.6   controlplane   <none>           <none>
blue-6b478c8dbf-rqkpn   1/1     Running   0          4m1s   10.244.0.5   controlplane   <none>           <none>

controlplane ~ ➜  
```

### How many pods are scheduled on node01 now in the default namespace?

```ruby
controlplane ~ ➜  kubectl get pod -o wide 
NAME                    READY   STATUS    RESTARTS   AGE    IP           NODE           NOMINATED NODE   READINESS GATES
blue-6b478c8dbf-7f9mx   1/1     Running   0          10m    10.244.0.4   controlplane   <none>           <none>
blue-6b478c8dbf-l77h6   1/1     Running   0          4m1s   10.244.0.6   controlplane   <none>           <none>
blue-6b478c8dbf-rqkpn   1/1     Running   0          4m1s   10.244.0.5   controlplane   <none>           <none>

```

### Why are there no pods on node01?

##### Running the ```uncordon``` command on a node will not automatically schedule pods on the node. When new pods are created, they will be placed on node01.

### Why are the pods placed on the controlplane node?

Check the controlplane node details.

    root@controlplane:~# kubectl describe node controlplane | grep -i  taint
    Taints:             <none>
    root@controlplane:~# 
    
##### Since there are no taints on the controlplane node, all the pods were started on it when we ran the kubectl drain node01 command.

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/a5c35249-2759-4212-adf6-ee43a6d4b23f)

```ruby
controlplane ~ ➜  kubectl describe nodes controlplane 
Name:               controlplane
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=controlplane
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"ba:8f:2d:15:86:cf"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 192.8.180.10
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 15 Feb 2024 00:08:00 -0500
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  controlplane
  AcquireTime:     <unset>
  RenewTime:       Thu, 15 Feb 2024 00:35:17 -0500
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Thu, 15 Feb 2024 00:08:37 -0500   Thu, 15 Feb 2024 00:08:37 -0500   FlannelIsUp                  Flannel is running on this node
  MemoryPressure       False   Thu, 15 Feb 2024 00:34:27 -0500   Thu, 15 Feb 2024 00:07:55 -0500   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Thu, 15 Feb 2024 00:34:27 -0500   Thu, 15 Feb 2024 00:07:55 -0500   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Thu, 15 Feb 2024 00:34:27 -0500   Thu, 15 Feb 2024 00:07:55 -0500   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Thu, 15 Feb 2024 00:34:27 -0500   Thu, 15 Feb 2024 00:08:34 -0500   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.8.180.10
  Hostname:    controlplane
Capacity:
  cpu:                36
  ephemeral-storage:  1016057248Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             214587048Ki
  pods:               110
Allocatable:
  cpu:                36
  ephemeral-storage:  936398358207
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             214484648Ki
  pods:               110
System Info:
  Machine ID:                 73d7539cb95c4ef09a8ddd274b5251bc
  System UUID:                e0179261-2681-56c9-cccb-28ec6ec80686
  Boot ID:                    c8e63dd2-1f19-4cf4-81b8-b2072dd2a56b
  Kernel Version:             5.4.0-1106-gcp
  OS Image:                   Ubuntu 20.04.6 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.6
  Kubelet Version:            v1.27.0
  Kube-Proxy Version:         v1.27.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  default                     blue-6b478c8dbf-7f9mx                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m
  default                     blue-6b478c8dbf-l77h6                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m9s
  default                     blue-6b478c8dbf-rqkpn                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m9s
  kube-flannel                kube-flannel-ds-wvc8z                   100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      26m
  kube-system                 coredns-5d78c9869d-4bmn8                100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     27m
  kube-system                 coredns-5d78c9869d-c64dh                100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     27m
  kube-system                 etcd-controlplane                       100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         27m
  kube-system                 kube-apiserver-controlplane             250m (0%)     0 (0%)      0 (0%)           0 (0%)         27m
  kube-system                 kube-controller-manager-controlplane    200m (0%)     0 (0%)      0 (0%)           0 (0%)         27m
  kube-system                 kube-proxy-qhlv5                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m
  kube-system                 kube-scheduler-controlplane             100m (0%)     0 (0%)      0 (0%)           0 (0%)         27m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (2%)   100m (0%)
  memory             290Mi (0%)  390Mi (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                From             Message
  ----     ------                   ----               ----             -------
  Normal   Starting                 26m                kube-proxy       
  Normal   NodeAllocatableEnforced  27m                kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                 27m                kubelet          Starting kubelet.
  Warning  InvalidDiskCapacity      27m                kubelet          invalid capacity 0 on image filesystem
  Normal   NodeHasSufficientMemory  27m (x8 over 27m)  kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal   NodeHasSufficientPID     27m (x7 over 27m)  kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal   NodeHasNoDiskPressure    27m (x7 over 27m)  kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal   RegisteredNode           27m                node-controller  Node controlplane event: Registered Node controlplane in Controller
  Normal   Starting                 27m                kubelet          Starting kubelet.
  Warning  InvalidDiskCapacity      27m                kubelet          invalid capacity 0 on image filesystem
  Normal   NodeAllocatableEnforced  27m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  27m                kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    27m                kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     27m                kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal   NodeReady                26m                kubelet          Node controlplane status is now: NodeReady

controlplane ~ ➜  
```


### Time travelling to the next maintenance window…

### We need to carry out a maintenance activity on node01 again. Try draining the node again using the same command as before: 

    kubectl drain node01 --ignore-daemonsets

Did that work?

```ruby
controlplane ~ ➜  kubectl drain node01 --ignore-daemonsets 
node/node01 cordoned
error: unable to drain node "node01" due to error:cannot delete Pods declare no controller (use --force to override): default/hr-app, continuing command...
There are pending nodes to be drained:
 node01
cannot delete Pods declare no controller (use --force to override): default/hr-app

controlplane ~ ✖ 
```
### Why did the drain command fail on node01? It worked the first time!

Run: ```kubectl get pods -o wide``` and you will see that there is a single pod scheduled on node01 which is not part of a replicaset.

The drain command will not work in this case. To forcefully drain the node we now have to use the ```--force``` flag.

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/4e610a34-8fb5-4c6a-b83c-be80126c6c85)

```ruby
controlplane ~ ➜  kubectl get pod -o wide
NAME                    READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
blue-6b478c8dbf-7f9mx   1/1     Running   0          30m     10.244.0.4   controlplane   <none>           <none>
blue-6b478c8dbf-l77h6   1/1     Running   0          24m     10.244.0.6   controlplane   <none>           <none>
blue-6b478c8dbf-rqkpn   1/1     Running   0          24m     10.244.0.5   controlplane   <none>           <none>
hr-app                  1/1     Running   0          5m12s   10.244.1.4   node01         <none>           <none>

controlplane ~ ➜  
controlplane ~ ✖ kubectl describe nodes node01 
Name:               node01
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node01
                    kubernetes.io/os=linux
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"1e:50:6c:81:8f:14"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 192.8.180.3
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 15 Feb 2024 00:08:38 -0500
Taints:             node.kubernetes.io/unschedulable:NoSchedule
Unschedulable:      true
Lease:
  HolderIdentity:  node01
  AcquireTime:     <unset>
  RenewTime:       Thu, 15 Feb 2024 00:49:01 -0500
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Thu, 15 Feb 2024 00:08:47 -0500   Thu, 15 Feb 2024 00:08:47 -0500   FlannelIsUp                  Flannel is running on this node
  MemoryPressure       False   Thu, 15 Feb 2024 00:45:40 -0500   Thu, 15 Feb 2024 00:08:38 -0500   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Thu, 15 Feb 2024 00:45:40 -0500   Thu, 15 Feb 2024 00:08:38 -0500   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Thu, 15 Feb 2024 00:45:40 -0500   Thu, 15 Feb 2024 00:08:38 -0500   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Thu, 15 Feb 2024 00:45:40 -0500   Thu, 15 Feb 2024 00:08:45 -0500   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.8.180.3
  Hostname:    node01
Capacity:
  cpu:                36
  ephemeral-storage:  1016057248Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             214587048Ki
  pods:               110
Allocatable:
  cpu:                36
  ephemeral-storage:  936398358207
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             214484648Ki
  pods:               110
System Info:
  Machine ID:                 2ec201a93f8a4623aa48bf37d16bd1d6
  System UUID:                e0179261-2681-56c9-cccb-95925277119d
  Boot ID:                    c8e63dd2-1f19-4cf4-81b8-b2072dd2a56b
  Kernel Version:             5.4.0-1106-gcp
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.6
  Kubelet Version:            v1.27.0
  Kube-Proxy Version:         v1.27.0
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
Non-terminated Pods:          (3 in total)
  Namespace                   Name                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                     ------------  ----------  ---------------  -------------  ---
  default                     hr-app                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m59s
  kube-flannel                kube-flannel-ds-ln4m4    100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      40m
  kube-system                 kube-proxy-544bd         0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (0%)  100m (0%)
  memory             50Mi (0%)  50Mi (0%)
  ephemeral-storage  0 (0%)     0 (0%)
  hugepages-1Gi      0 (0%)     0 (0%)
  hugepages-2Mi      0 (0%)     0 (0%)
Events:
  Type     Reason                   Age                 From             Message
  ----     ------                   ----                ----             -------
  Normal   Starting                 40m                 kube-proxy       
  Normal   Starting                 40m                 kubelet          Starting kubelet.
  Warning  InvalidDiskCapacity      40m                 kubelet          invalid capacity 0 on image filesystem
  Normal   NodeHasSufficientMemory  40m                 kubelet          Node node01 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    40m                 kubelet          Node node01 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     40m                 kubelet          Node node01 status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  40m                 kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           40m                 node-controller  Node node01 event: Registered Node node01 in Controller
  Normal   NodeReady                40m                 kubelet          Node node01 status is now: NodeReady
  Normal   NodeSchedulable          18m                 kubelet          Node node01 status is now: NodeSchedulable
  Normal   NodeNotSchedulable       103s (x2 over 22m)  kubelet          Node node01 status is now: NodeNotSchedulable

controlplane ~ ➜  
```

#### What is the name of the POD hosted on node01 that is not part of a replicaset?

```ruby
controlplane ~ ➜  kubectl get pod -o wide
NAME                    READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
blue-6b478c8dbf-7f9mx   1/1     Running   0          30m     10.244.0.4   controlplane   <none>           <none>
blue-6b478c8dbf-l77h6   1/1     Running   0          24m     10.244.0.6   controlplane   <none>           <none>
blue-6b478c8dbf-rqkpn   1/1     Running   0          24m     10.244.0.5   controlplane   <none>           <none>
hr-app                  1/1     Running   0          5m12s   10.244.1.4   node01         <none>           <none>

```
### What would happen to hr-app if node01 is drained forcefully?

Try it and see for yourself.

    hr app will lost forever

A forceful drain of the node will delete any pod that is not part of a replicaset.

```ruby
controlplane ~ ➜  kubectl drain node01 --ignore-daemonsets --force
node/node01 already cordoned
Warning: deleting Pods that declare no controller: default/hr-app; ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-ln4m4, kube-system/kube-proxy-544bd
evicting pod default/hr-app
pod/hr-app evicted
node/node01 drained

controlplane ~ ➜  kubectl get pod -o wide 
NAME                    READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
blue-6b478c8dbf-7f9mx   1/1     Running   0          37m   10.244.0.4   controlplane   <none>           <none>
blue-6b478c8dbf-l77h6   1/1     Running   0          30m   10.244.0.6   controlplane   <none>           <none>
blue-6b478c8dbf-rqkpn   1/1     Running   0          30m   10.244.0.5   controlplane   <none>           <none>

controlplane ~ ➜  kubectl get nodes 
NAME           STATUS                     ROLES           AGE   VERSION
controlplane   Ready                      control-plane   49m   v1.27.0
node01         Ready,SchedulingDisabled   <none>          48m   v1.27.0

```

#### Oops! We did not want to do that! hr-app is a critical application that should not be destroyed. We have now reverted back to the previous state and re-deployed hr-app as a deployment.

### hr-app is a critical app and we do not want it to be removed and we do not want to schedule any more pods on node01.
Mark node01 as unschedulable so that no new pods are scheduled on this node.


Make sure that hr-app is not affected.




    Node01 Unschedulable
    
    hr-app still running on node01?


Do not drain node01, instead use the ```kubectl cordon node01``` command. This will ensure that no new pods are scheduled on this node and the existing pods will not be affected by this operation.

```ruby
controlplane ~ ✖ kubectl cordon node01 
node/node01 cordoned

controlplane ~ ➜  kubectl get pod -o wide 
NAME                     READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
blue-6b478c8dbf-7f9mx    1/1     Running   0          41m     10.244.0.4   controlplane   <none>           <none>
blue-6b478c8dbf-l77h6    1/1     Running   0          35m     10.244.0.6   controlplane   <none>           <none>
blue-6b478c8dbf-rqkpn    1/1     Running   0          35m     10.244.0.5   controlplane   <none>           <none>
hr-app-6d6df76fc-277s4   1/1     Running   0          2m19s   10.244.1.5   node01         <none>           <none>

controlplane ~ ➜  kubectl get nodes 
NAME           STATUS                     ROLES           AGE   VERSION
controlplane   Ready                      control-plane   53m   v1.27.0
node01         Ready,SchedulingDisabled   <none>          53m   v1.27.0

controlplane ~ ➜  k describe nodes node01 
Name:               node01
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node01
                    kubernetes.io/os=linux
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"1e:50:6c:81:8f:14"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 192.8.180.3
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 15 Feb 2024 00:08:38 -0500
Taints:             node.kubernetes.io/unschedulable:NoSchedule
Unschedulable:      true
Lease:
  HolderIdentity:  node01
  AcquireTime:     <unset>
  RenewTime:       Thu, 15 Feb 2024 01:02:41 -0500
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Thu, 15 Feb 2024 00:08:47 -0500   Thu, 15 Feb 2024 00:08:47 -0500   FlannelIsUp                  Flannel is running on this node
  MemoryPressure       False   Thu, 15 Feb 2024 01:01:01 -0500   Thu, 15 Feb 2024 00:08:38 -0500   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Thu, 15 Feb 2024 01:01:01 -0500   Thu, 15 Feb 2024 00:08:38 -0500   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Thu, 15 Feb 2024 01:01:01 -0500   Thu, 15 Feb 2024 00:08:38 -0500   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Thu, 15 Feb 2024 01:01:01 -0500   Thu, 15 Feb 2024 00:08:45 -0500   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.8.180.3
  Hostname:    node01
Capacity:
  cpu:                36
  ephemeral-storage:  1016057248Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             214587048Ki
  pods:               110
Allocatable:
  cpu:                36
  ephemeral-storage:  936398358207
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             214484648Ki
  pods:               110
System Info:
  Machine ID:                 2ec201a93f8a4623aa48bf37d16bd1d6
  System UUID:                e0179261-2681-56c9-cccb-95925277119d
  Boot ID:                    c8e63dd2-1f19-4cf4-81b8-b2072dd2a56b
  Kernel Version:             5.4.0-1106-gcp
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.6
  Kubelet Version:            v1.27.0
  Kube-Proxy Version:         v1.27.0
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
Non-terminated Pods:          (3 in total)
  Namespace                   Name                      CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                      ------------  ----------  ---------------  -------------  ---
  default                     hr-app-6d6df76fc-277s4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m42s
  kube-flannel                kube-flannel-ds-ln4m4     100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      54m
  kube-system                 kube-proxy-544bd          0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (0%)  100m (0%)
  memory             50Mi (0%)  50Mi (0%)
  ephemeral-storage  0 (0%)     0 (0%)
  hugepages-1Gi      0 (0%)     0 (0%)
  hugepages-2Mi      0 (0%)     0 (0%)
Events:
  Type     Reason                   Age                  From             Message
  ----     ------                   ----                 ----             -------
  Normal   Starting                 54m                  kube-proxy       
  Normal   Starting                 54m                  kubelet          Starting kubelet.
  Warning  InvalidDiskCapacity      54m                  kubelet          invalid capacity 0 on image filesystem
  Normal   NodeHasSufficientMemory  54m                  kubelet          Node node01 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    54m                  kubelet          Node node01 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     54m                  kubelet          Node node01 status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  54m                  kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           54m                  node-controller  Node node01 event: Registered Node node01 in Controller
  Normal   NodeReady                53m                  kubelet          Node node01 status is now: NodeReady
  Normal   NodeSchedulable          3m34s (x2 over 32m)  kubelet          Node node01 status is now: NodeSchedulable
  Normal   NodeNotSchedulable       91s (x3 over 36m)    kubelet          Node node01 status is now: NodeNotSchedulable

controlplane ~ ➜  
```

