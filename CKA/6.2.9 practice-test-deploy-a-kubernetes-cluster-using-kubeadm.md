### Install the ```kubeadm``` and ```kubelet``` packages on the ```controlplane``` and ```node01``` nodes.

Use the exact version of ```1.29.0-1.1``` for both.


Refer to the official k8s documentation - 

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

and follow the installation steps.

-----

These steps have to be performed on both nodes.

```set net.bridge.bridge-nf-call-iptables``` to```1```:

    cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
    br_netfilter
    EOF
    
    cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
    net.bridge.bridge-nf-call-ip6tables = 1
    net.bridge.bridge-nf-call-iptables = 1
    EOF
    
    sudo sysctl --system

The container runtime has already been installed on both nodes, so you may skip this step.
Install ```kubeadm```, ```kubectl``` and ```kubelet``` on all nodes:


    sudo apt-get update
    sudo apt-get install -y apt-transport-https ca-certificates curl
    
    sudo mkdir -m 755 /etc/apt/keyrings
    
    curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
    
    echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
    
    sudo apt-get update
    
    # To see the new version labels
    sudo apt-cache madison kubeadm
    
    sudo apt-get install -y kubelet=1.29.0-1.1 kubeadm=1.29.0-1.1 kubectl=1.29.0-1.1
    
    sudo apt-mark hold kubelet kubeadm kubectl




on Masternode

    
```ruby

controlplane ~ ➜  cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
> br_netfilter
> EOF
br_netfilter

controlplane ~ ➜  

controlplane ~ ➜  cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
> net.bridge.bridge-nf-call-ip6tables = 1
> net.bridge.bridge-nf-call-iptables = 1
> EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

controlplane ~ ➜  

controlplane ~ ➜  sudo sysctl --system
* Applying /etc/sysctl.d/10-console-messages.conf ...
kernel.printk = 4 4 1 7
* Applying /etc/sysctl.d/10-ipv6-privacy.conf ...
net.ipv6.conf.all.use_tempaddr = 2
net.ipv6.conf.default.use_tempaddr = 2
* Applying /etc/sysctl.d/10-kernel-hardening.conf ...
kernel.kptr_restrict = 1
* Applying /etc/sysctl.d/10-link-restrictions.conf ...
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/10-magic-sysrq.conf ...
kernel.sysrq = 176
* Applying /etc/sysctl.d/10-network-security.conf ...
net.ipv4.conf.default.rp_filter = 2
net.ipv4.conf.all.rp_filter = 2
* Applying /etc/sysctl.d/10-ptrace.conf ...
kernel.yama.ptrace_scope = 1
* Applying /etc/sysctl.d/10-zeropage.conf ...
vm.mmap_min_addr = 65536
* Applying /usr/lib/sysctl.d/50-default.conf ...
net.ipv4.conf.default.promote_secondaries = 1
sysctl: setting key "net.ipv4.conf.all.promote_secondaries": Invalid argument
net.ipv4.ping_group_range = 0 2147483647
net.core.default_qdisc = fq_codel
sysctl: permission denied on key "fs.protected_regular", ignoring
sysctl: permission denied on key "fs.protected_fifos", ignoring
* Applying /usr/lib/sysctl.d/50-pid-max.conf ...
kernel.pid_max = 4194304
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
* Applying /usr/lib/sysctl.d/protect-links.conf ...
sysctl: permission denied on key "fs.protected_fifos", ignoring
fs.protected_hardlinks = 1
sysctl: permission denied on key "fs.protected_regular", ignoring
fs.protected_symlinks = 1
* Applying /etc/sysctl.conf ...

controlplane ~ ➜  sudo apt-get update
Get:1 https://download.docker.com/linux/ubuntu focal InRelease [57.7 kB]
Get:2 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]       
Get:3 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]
Get:4 https://download.docker.com/linux/ubuntu focal/stable amd64 Packages [45.0 kB]
Get:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]                   
Get:6 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.7 kB]
Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]
Get:8 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3,327 kB]
Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]
Get:10 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,187 kB]
Get:11 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3,421 kB]    
Get:12 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]          
Get:13 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]
Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1,275 kB]
Get:15 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.4 kB]
Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,895 kB]
Get:17 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3,477 kB]
Get:18 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,483 kB]
Get:19 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]
Get:20 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]
Fetched 30.5 MB in 3s (10.3 MB/s)                            
Reading package lists... Done

controlplane ~ ➜  sudo apt-get install -y apt-transport-https ca-certificates curl
Reading package lists... Done
Building dependency tree       
Reading state information... Done
ca-certificates is already the newest version (20230311ubuntu0.20.04.1).
The following packages were automatically installed and are no longer required:
  conntrack cri-tools ebtables ethtool kubernetes-cni socat
Use 'sudo apt autoremove' to remove them.
The following packages will be upgraded:
  apt-transport-https curl libcurl4
3 upgraded, 0 newly installed, 0 to remove and 63 not upgraded.
Need to get 398 kB of archives.
After this operation, 0 B of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 apt-transport-https all 2.0.10 [1,704 B]
Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 curl amd64 7.68.0-1ubuntu2.21 [161 kB]
Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libcurl4 amd64 7.68.0-1ubuntu2.21 [235 kB]
Fetched 398 kB in 0s (1,148 kB/s) 
debconf: delaying package configuration, since apt-utils is not installed
(Reading database ... 20459 files and directories currently installed.)
Preparing to unpack .../apt-transport-https_2.0.10_all.deb ...
Unpacking apt-transport-https (2.0.10) over (2.0.9) ...
Preparing to unpack .../curl_7.68.0-1ubuntu2.21_amd64.deb ...
Unpacking curl (7.68.0-1ubuntu2.21) over (7.68.0-1ubuntu2.20) ...
Preparing to unpack .../libcurl4_7.68.0-1ubuntu2.21_amd64.deb ...
Unpacking libcurl4:amd64 (7.68.0-1ubuntu2.21) over (7.68.0-1ubuntu2.20) ...
Setting up apt-transport-https (2.0.10) ...
Setting up libcurl4:amd64 (7.68.0-1ubuntu2.21) ...
Setting up curl (7.68.0-1ubuntu2.21) ...
Processing triggers for man-db (2.9.1-1) ...
Processing triggers for libc-bin (2.31-0ubuntu9.12) ...

controlplane ~ ➜  
```
On Node

```ruby
controlplane ~ ➜  ssh node01
Warning: Permanently added the ECDSA host key for IP address '192.8.42.10' to the list of known hosts.

root@node01 ~ ➜  cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
> br_netfilter
> EOF
br_netfilter

root@node01 ~ ➜  

root@node01 ~ ➜  cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
> net.bridge.bridge-nf-call-ip6tables = 1
> net.bridge.bridge-nf-call-iptables = 1
> EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

root@node01 ~ ➜  

root@node01 ~ ➜  sudo sysctl --system
* Applying /etc/sysctl.d/10-console-messages.conf ...
kernel.printk = 4 4 1 7
* Applying /etc/sysctl.d/10-ipv6-privacy.conf ...
net.ipv6.conf.all.use_tempaddr = 2
net.ipv6.conf.default.use_tempaddr = 2
* Applying /etc/sysctl.d/10-kernel-hardening.conf ...
kernel.kptr_restrict = 1
* Applying /etc/sysctl.d/10-link-restrictions.conf ...
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/10-magic-sysrq.conf ...
kernel.sysrq = 176
* Applying /etc/sysctl.d/10-network-security.conf ...
net.ipv4.conf.default.rp_filter = 2
net.ipv4.conf.all.rp_filter = 2
* Applying /etc/sysctl.d/10-ptrace.conf ...
kernel.yama.ptrace_scope = 1
* Applying /etc/sysctl.d/10-zeropage.conf ...
vm.mmap_min_addr = 65536
* Applying /usr/lib/sysctl.d/50-default.conf ...
net.ipv4.conf.default.promote_secondaries = 1
sysctl: setting key "net.ipv4.conf.all.promote_secondaries": Invalid argument
net.ipv4.ping_group_range = 0 2147483647
net.core.default_qdisc = fq_codel
sysctl: permission denied on key "fs.protected_regular", ignoring
sysctl: permission denied on key "fs.protected_fifos", ignoring
* Applying /usr/lib/sysctl.d/50-pid-max.conf ...
kernel.pid_max = 4194304
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
* Applying /usr/lib/sysctl.d/protect-links.conf ...
sysctl: permission denied on key "fs.protected_fifos", ignoring
fs.protected_hardlinks = 1
sysctl: permission denied on key "fs.protected_regular", ignoring
fs.protected_symlinks = 1
* Applying /etc/sysctl.conf ...

root@node01 ~ ➜  sudo apt-get update
Get:1 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]
Get:2 https://download.docker.com/linux/ubuntu focal InRelease [57.7 kB]
Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]              
Get:4 https://download.docker.com/linux/ubuntu focal/stable amd64 Packages [45.0 kB]
Get:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]
Get:6 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]
Get:7 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]
Get:8 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]
Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]
Get:10 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.7 kB]
Get:11 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]
Get:12 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3327 kB]
Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3895 kB]       
Get:14 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1187 kB] 
Get:15 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3421 kB]     
Get:16 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3477 kB] 
Get:17 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.4 kB] 
Get:18 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1483 kB]
Get:19 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]
Get:20 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]
Fetched 30.5 MB in 3s (10.3 MB/s)                            
Reading package lists... Done

root@node01 ~ ➜  sudo apt-get install -y apt-transport-https ca-certificates curl
Reading package lists... Done
Building dependency tree       
Reading state information... Done
ca-certificates is already the newest version (20230311ubuntu0.20.04.1).
The following packages were automatically installed and are no longer required:
  conntrack cri-tools ebtables ethtool kubernetes-cni socat
Use 'sudo apt autoremove' to remove them.
The following packages will be upgraded:
  apt-transport-https curl libcurl4
3 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.
Need to get 398 kB of archives.
After this operation, 0 B of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 apt-transport-https all 2.0.10 [1704 B]
Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 curl amd64 7.68.0-1ubuntu2.21 [161 kB]
Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libcurl4 amd64 7.68.0-1ubuntu2.21 [235 kB]
Fetched 398 kB in 0s (1334 kB/s) 
debconf: delaying package configuration, since apt-utils is not installed
(Reading database ... 14836 files and directories currently installed.)
Preparing to unpack .../apt-transport-https_2.0.10_all.deb ...
Unpacking apt-transport-https (2.0.10) over (2.0.9) ...
Preparing to unpack .../curl_7.68.0-1ubuntu2.21_amd64.deb ...
Unpacking curl (7.68.0-1ubuntu2.21) over (7.68.0-1ubuntu2.19) ...
Preparing to unpack .../libcurl4_7.68.0-1ubuntu2.21_amd64.deb ...
Unpacking libcurl4:amd64 (7.68.0-1ubuntu2.21) over (7.68.0-1ubuntu2.19) ...
Setting up apt-transport-https (2.0.10) ...
Setting up libcurl4:amd64 (7.68.0-1ubuntu2.21) ...
Setting up curl (7.68.0-1ubuntu2.21) ...
Processing triggers for libc-bin (2.31-0ubuntu9.9) ...

root@node01 ~ ➜

```


### What is the version of ```kubelet``` installed?


Run: ```kubelet --version``` on the ```controlplane``` node.


```ruby
controlplane ~ ➜  kubelet --version
Kubernetes v1.29.0

controlplane ~ ➜  
```
### How many nodes are part of kubernetes cluster currently?

Are you able to run ```kubectl get nodes```?

```ruby

controlplane ~ ➜  kubectl get nodes
E0306 00:19:24.798332   15191 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:19:24.799708   15191 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:19:24.800910   15191 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:19:24.802083   15191 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:19:24.802982   15191 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
Error from server (NotFound): the server could not find the requested resource

controlplane ~ ✖ 
```

### Lets now bootstrap a ```kubernetes``` cluster using ```kubeadm```.


The latest version of Kubernetes will be installed.


### Initialize Control Plane Node (Master Node). Use the following options:


    1.apiserver-advertise-address - Use the IP address allocated to eth0 on the controlplane node
    
    2.apiserver-cert-extra-sans - Set it to controlplane
    
    3.pod-network-cidr - Set to 10.244.0.0/16

Once done, set up the default kubeconfig file and wait for node to be part of the cluster.


run 

    kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address 192.83.79.10 --pod-network-cidr=10.244.0.0/16

The IP address used here is just an example. It will change for your lab session. Make sure to check the IP address allocated to eth0 by running:

    root@controlplane:~# ifconfig eth0
    eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450
            inet 192.83.79.10  netmask 255.255.255.0  broadcast 192.83.79.255
            ether 02:42:c0:17:6c:08  txqueuelen 0  (Ethernet)
            RX packets 3491  bytes 466178 (466.1 KB)
            RX errors 0  dropped 0  overruns 0  frame 0
            TX packets 3371  bytes 939716 (939.7 KB)
            TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
    
    root@controlplane:~#

    
In this example, the IP address is 192.83.79.10



solution

You can use the below ```kubeadm init``` command to spin up the cluster:


    IP_ADDR=$(ip addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
    kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address $IP_ADDR --pod-network-cidr=10.244.0.0/16


Once you run the init command, you should see an output similar to below:


    [init] Using Kubernetes version: v1.29.1
    [preflight] Running pre-flight checks
    [preflight] Pulling images required for setting up a Kubernetes cluster
    [preflight] This might take a minute or two, depending on the speed of your internet connection
    [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
    W0127 16:19:04.194710   15691 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.6" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
    [certs] Using certificateDir folder "/etc/kubernetes/pki"
    [certs] Generating "ca" certificate and key
    [certs] Generating "apiserver" certificate and key
    [certs] apiserver serving cert is signed for DNS names [controlplane kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.83.79.10]
    [certs] Generating "apiserver-kubelet-client" certificate and key
    [certs] Generating "front-proxy-ca" certificate and key
    [certs] Generating "front-proxy-client" certificate and key
    [certs] Generating "etcd/ca" certificate and key
    [certs] Generating "etcd/server" certificate and key
    [certs] etcd/server serving cert is signed for DNS names [controlplane localhost] and IPs [192.83.79.10 127.0.0.1 ::1]
    [certs] Generating "etcd/peer" certificate and key
    [certs] etcd/peer serving cert is signed for DNS names [controlplane localhost] and IPs [192.83.79.10 127.0.0.1 ::1]
    [certs] Generating "etcd/healthcheck-client" certificate and key
    [certs] Generating "apiserver-etcd-client" certificate and key
    [certs] Generating "sa" key and public key
    [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
    [kubeconfig] Writing "admin.conf" kubeconfig file
    [kubeconfig] Writing "super-admin.conf" kubeconfig file
    [kubeconfig] Writing "kubelet.conf" kubeconfig file
    [kubeconfig] Writing "controller-manager.conf" kubeconfig file
    [kubeconfig] Writing "scheduler.conf" kubeconfig file
    [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
    [control-plane] Using manifest folder "/etc/kubernetes/manifests"
    [control-plane] Creating static Pod manifest for "kube-apiserver"
    [control-plane] Creating static Pod manifest for "kube-controller-manager"
    [control-plane] Creating static Pod manifest for "kube-scheduler"
    [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
    [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
    [kubelet-start] Starting the kubelet
    [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
    [apiclient] All control plane components are healthy after 24.532497 seconds
    [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
    [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
    [upload-certs] Skipping phase. Please see --upload-certs
    [mark-control-plane] Marking the node controlplane as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
    [mark-control-plane] Marking the node controlplane as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
    [bootstrap-token] Using token: 5tz3lo.v3qjgkjb7qg0k9uw
    [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
    [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
    [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
    [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
    [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
    [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
    [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
    [addons] Applied essential addon: CoreDNS
    [addons] Applied essential addon: kube-proxy
    
    Your Kubernetes control-plane has initialized successfully!
    
    To start using your cluster, you need to run the following as a regular user:
    
      mkdir -p $HOME/.kube
      sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
      sudo chown $(id -u):$(id -g) $HOME/.kube/config
    
    Alternatively, if you are the root user, you can run:
    
      export KUBECONFIG=/etc/kubernetes/admin.conf
    
    You should now deploy a pod network to the cluster.
    Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
      https://kubernetes.io/docs/concepts/cluster-administration/addons/
    
    Then you can join any number of worker nodes by running the following on each as root:
    
    kubeadm join 192.83.79.10:6443 --token 5tz3lo.v3qjgkjb7qg0k9uw \
            --discovery-token-ca-cert-hash sha256:bf2c7531e52e959c02a63dbd6f9385bf2385d4d796667ff7f9a9386d01346289 


Once the command has been run successfully, set up the kubeconfig:

    root@controlplane:~> mkdir -p $HOME/.kube
    root@controlplane:~> sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    root@controlplane:~> sudo chown $(id -u):$(id -g) $HOME/.kube/config
    root@controlplane:~>


```ruby
controlplane ~ ➜  kubectl get nodes
E0306 00:19:24.798332   15191 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:19:24.799708   15191 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:19:24.800910   15191 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:19:24.802083   15191 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:19:24.802982   15191 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
Error from server (NotFound): the server could not find the requested resource

controlplane ~ ✖ IP_ADDR=$(ip addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')

controlplane ~ ➜  kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address $IP_ADDR --pod-network-cidr=10.244.0.0/16
[init] Using Kubernetes version: v1.29.2
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0306 00:30:03.789152   15295 checks.go:835] detected that the sandbox image "k8s.gcr.io/pause:3.6" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [controlplane kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.8.42.8]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [controlplane localhost] and IPs [192.8.42.8 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [controlplane localhost] and IPs [192.8.42.8 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.002590 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node controlplane as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node controlplane as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: qjkdoy.yd263qub2h03i66e
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.8.42.8:6443 --token qjkdoy.yd263qub2h03i66e \
        --discovery-token-ca-cert-hash sha256:b2b385e3786571816a938e5e6759c077f0046695fcaf141e75ccf17d64ad5db1 

controlplane ~ ➜  kubectl get nodes
E0306 00:30:56.336767   18276 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:30:56.338156   18276 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:30:56.339307   18276 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:30:56.340548   18276 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:30:56.342033   18276 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
Error from server (NotFound): the server could not find the requested resource

controlplane ~ ✖ kubectl get pod
E0306 00:31:13.770581   18349 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:31:13.772057   18349 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:31:13.773721   18349 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:31:13.774776   18349 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
E0306 00:31:13.775867   18349 memcache.go:265] couldn't get current server API group list: the server could not find the requested resource
Error from server (NotFound): the server could not find the requested resource

controlplane ~ ✖ mkdir -p $HOME/.kube

controlplane ~ ➜    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

controlplane ~ ➜    sudo chown $(id -u):$(id -g) $HOME/.kube/config

controlplane ~ ➜  kubectl get pod
No resources found in default namespace.

controlplane ~ ➜  kubectl get nodes
NAME           STATUS     ROLES           AGE   VERSION
controlplane   NotReady   control-plane   70s   v1.29.0

controlplane ~ ➜  kubectl get nodes
NAME           STATUS     ROLES           AGE     VERSION
controlplane   NotReady   control-plane   3m41s   v1.29.0

controlplane ~ ➜

controlplane ~ ➜
controlplane ~ ➜  ifconfig eth0
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450
        inet 192.8.42.8  netmask 255.255.255.0  broadcast 192.8.42.255
        ether 02:42:c0:08:2a:08  txqueuelen 0  (Ethernet)
        RX packets 6379  bytes 751593 (751.5 KB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 6027  bytes 2393662 (2.3 MB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0


controlplane ~ ➜ 


```


### Generate a kubeadm join token

Or copy the one that was generated by ```kubeadm init``` command

    kubeadm join 192.8.42.8:6443 --token qjkdoy.yd263qub2h03i66e \
        --discovery-token-ca-cert-hash sha256:b2b385e3786571816a938e5e6759c077f0046695fcaf141e75ccf17d64ad5db1

### Join node01 to the cluster using the join token

Use the join token provided by the kubeadm command or create a new token


To create token:


    root@controlplane:~> kubeadm token create --print-join-command
    kubeadm join 192.83.79.10:6443 --token f4gugx.u9rfo93lz1qeiy32 --discovery-token-ca-cert-hash sha256:bf2c7531e52e959c02a63dbd6f9385bf2385d4d796667ff7f9a9386d01346289


next, SSH to the ```node01``` and run the join command on the terminal:

    root@node01:~> kubeadm join 192.83.79.10:6443 --token f4gugx.u9rfo93lz1qeiy32 --discovery-token-ca-cert-hash sha256:bf2c7531e52e959c02a63dbd6f9385bf2385d4d796667ff7f9a9386d01346289 
    [preflight] Running pre-flight checks
    [preflight] Reading configuration from the cluster...
    [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
    [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
    [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
    [kubelet-start] Starting the kubelet
    [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
    
    This node has joined the cluster:
    * Certificate signing request was sent to apiserver and a response was received.
    * The Kubelet was informed of the new secure connection details.
    
    Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

### On Node01

```ruby
root@node01 ~ ➜  kubeadm join 192.8.42.8:6443 --token qjkdoy.yd263qub2h03i66e \
>         --discovery-token-ca-cert-hash sha256:b2b385e3786571816a938e5e6759c077f0046695fcaf141e75ccf17d64ad5db1
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.


root@node01 ~ ➜  
```
### on controlplane node 

```ruby
controlplane ~ ➜  kubectl get nodes
NAME           STATUS     ROLES           AGE     VERSION
controlplane   NotReady   control-plane   6m44s   v1.29.0
node01         NotReady   <none>          66s     v1.29.0

controlplane ~ ➜  kubectl get pod
No resources found in default namespace.


```


### To install a ```network plugin```, we will go with Flannel as the default choice. For ```inter-host communication```, we will utilize the ```eth0``` interface.


Please ensure that the ```Flannel``` manifest includes the appropriate options for this configuration.


Refer to the official documentation for the procedure.


Install ```flannel CNI``` and make sure to specify the interface to ```eth0```.



On the controlplane node, run the following set of commands to deploy the network plugin:
Download the original YAML file and save it as kube-flannel.yml:
    curl -LO https://raw.githubusercontent.com/flannel-io/flannel/v0.20.2/Documentation/kube-flannel.yml
Open the kube-flannel.yml file using a text editor.

Locate the args section within the kube-flannel container definition. It should look like this:
    
      args:
      - --ip-masq
      - --kube-subnet-mgr
      
Add the additional argument`` - --iface=eth0``` to the existing list of arguments.

Now apply the modified manifest kube-flannel.yml file using kubectl:

    kubectl apply -f kube-flannel.yml
    
After applying the manifest, the STATUS of both the nodes should become Ready

    controlplane ~ ➜  kubectl get nodes
    NAME           STATUS   ROLES           AGE   VERSION
    controlplane   Ready    control-plane   15m   v1.29.0
    node01         Ready    <none>          15m   v1.29.0


![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/64df76a1-7596-4257-9744-22e9f728dd7d)
