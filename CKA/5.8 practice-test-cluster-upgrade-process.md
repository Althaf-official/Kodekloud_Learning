#### This lab tests your skills on upgrading a kubernetes cluster. We have a production cluster with applications running on it. Let us explore the setup first.

What is the current version of the cluster?

```ruby
controlplane ~ ➜  kubectl get node
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   69m   v1.26.0
node01         Ready    <none>          69m   v1.26.0
```

#### How many nodes are part of this cluster?

Including controlplane and worker nodes


2

#### How many nodes can host workloads in this cluster?

Inspect the applications and taints set on the nodes.


Check the ```taints``` on both controlplane and node01. If none exists, then both nodes can host workloads.

By running the kubectl describe node command, we can see that neither nodes have taints.

    root@controlplane:~# kubectl describe nodes  controlplane | grep -i taint
    Taints:             <none>
    root@controlplane:~# 
    root@controlplane:~# kubectl describe nodes  node01 | grep -i taint
    Taints:             <none>
    root@controlplane:~# 

    
This means that both nodes have the ability to schedule workloads on them.

```ruby
controlplane ~ ➜  kubectl describe nodes controlplane | grep -i taint
Taints:             <none>

controlplane ~ ➜  kubectl describe nodes node01 | grep -i taint
Taints:             <none>

controlplane ~ ➜  
```

### How many applications are hosted on the cluster?

Count the number of deployments in the default namespace.

```ruby
controlplane ~ ➜  kubectl get deployments.apps 
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   5/5     5            5           5m27s

controlplane ~ ➜  
```
#### What nodes are the pods hosted on?

```ruby
controlplane ~ ➜  kubectl get pod -o wide 
NAME                   READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
blue-987f68cb5-bdfnr   1/1     Running   0          6m28s   10.244.0.5   controlplane   <none>           <none>
blue-987f68cb5-f6d4k   1/1     Running   0          6m28s   10.244.1.3   node01         <none>           <none>
blue-987f68cb5-jzcm8   1/1     Running   0          6m28s   10.244.1.4   node01         <none>           <none>
blue-987f68cb5-mg46s   1/1     Running   0          6m28s   10.244.0.4   controlplane   <none>           <none>
blue-987f68cb5-znjq7   1/1     Running   0          6m28s   10.244.1.2   node01         <none>           <none>

controlplane ~ ➜  
```

### You are tasked to upgrade the cluster. Users accessing the applications must not be impacted, and you cannot provision new VMs. What strategy would you use to upgrade the cluster?

    In order to ensure minimum downtime, upgrade the cluster one node at a time, while moving the workloads to another node.
    In the upcoming tasks you will get to practice how to do that.

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/bd9ed90d-6c1d-4fb7-8d8c-371ab2a3df86)


## What is the latest stable version of Kubernetes as of today?

#### Look at the ```remote version``` in the output of the ```kubeadm upgrade plan``` command.

```ruby

controlplane ~ ➜  kubeadm upgrade plan
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.26.0
[upgrade/versions] kubeadm version: v1.26.0
I0215 03:18:58.886626   18492 version.go:256] remote version is much newer: v1.29.2; falling back to: stable-1.26
[upgrade/versions] Target version: v1.26.14
[upgrade/versions] Latest version in the v1.26 series: v1.26.14

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       TARGET
kubelet     2 x v1.26.0   v1.26.14

Upgrade to the latest version in the v1.26 series:

COMPONENT                 CURRENT   TARGET
kube-apiserver            v1.26.0   v1.26.14
kube-controller-manager   v1.26.0   v1.26.14
kube-scheduler            v1.26.0   v1.26.14
kube-proxy                v1.26.0   v1.26.14
CoreDNS                   v1.9.3    v1.9.3
etcd                      3.5.6-0   3.5.6-0

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.26.14

Note: Before you can perform this upgrade, you have to update kubeadm to v1.26.14.

_____________________________________________________________________


The table below shows the current state of component configs as understood by this version of kubeadm.
Configs that have a "yes" mark in the "MANUAL UPGRADE REQUIRED" column require manual config upgrade or
resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually
upgrade to is denoted in the "PREFERRED VERSION" column.

API GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED
kubeproxy.config.k8s.io   v1alpha1          v1alpha1            no
kubelet.config.k8s.io     v1beta1           v1beta1             no
_____________________________________________________________________


controlplane ~ ➜
controlplane ~ ➜  kubeadm upgrade plan | grep -i remote
I0215 03:20:35.403431   18748 version.go:256] remote version is much newer: v1.29.2; falling back to: stable-1.26

```

Kubeadm is a tool provided by Kubernetes to streamline the process of setting up and managing Kubernetes clusters. It is primarily designed to help bootstrap Kubernetes clusters by automating many of the manual processes involved in setting up the control plane components and worker nodes.

Here's an overview of what kubeadm does:

1. **Bootstrap Control Plane**: Kubeadm assists in setting up the control plane components (like the API server, controller manager, and scheduler) on the master node(s). It automates tasks such as generating TLS certificates, configuring the API server, setting up the networking, and initializing the cluster state in etcd.

2. **Join Worker Nodes**: Kubeadm provides commands to help worker nodes join the cluster. It simplifies the process of configuring worker nodes by generating the necessary kubelet configuration and joining tokens.

3. **Configuration Management**: Kubeadm helps manage the cluster's configuration by providing commands to update, view, and validate the cluster's configuration.

4. **Upgrades**: Kubeadm assists in upgrading Kubernetes clusters to newer versions by providing commands to handle the upgrade process, including upgrading the control plane components and worker nodes.

5. **TLS Bootstrap**: Kubeadm handles the generation of TLS certificates and keys required for secure communication between Kubernetes components.

Kubeadm is especially useful for administrators or developers who need to quickly set up Kubernetes clusters for development, testing, or production environments. It abstracts away much of the complexity involved in configuring the cluster, making it easier to get started with Kubernetes.

It's important to note that while kubeadm automates many aspects of cluster setup, it's still important to understand the underlying concepts and components of Kubernetes for effective cluster management and troubleshooting.

#### What is the latest version available for an upgrade with the current version of the kubeadm tool installed?

Use the kubeadm tool

```ruby
controlplane ~ ✖ kubeadm upgrade plan
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.26.0
[upgrade/versions] kubeadm version: v1.26.0
I0215 03:27:38.873383   19697 version.go:256] remote version is much newer: v1.29.2; falling back to: stable-1.26
[upgrade/versions] Target version: v1.26.14
[upgrade/versions] Latest version in the v1.26 series: v1.26.14

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       TARGET
kubelet     2 x v1.26.0   v1.26.14

Upgrade to the latest version in the v1.26 series:

COMPONENT                 CURRENT   TARGET
kube-apiserver            v1.26.0   v1.26.14
kube-controller-manager   v1.26.0   v1.26.14
kube-scheduler            v1.26.0   v1.26.14
kube-proxy                v1.26.0   v1.26.14
CoreDNS                   v1.9.3    v1.9.3
etcd                      3.5.6-0   3.5.6-0

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.26.14

Note: Before you can perform this upgrade, you have to update kubeadm to v1.26.14.

_____________________________________________________________________


The table below shows the current state of component configs as understood by this version of kubeadm.
Configs that have a "yes" mark in the "MANUAL UPGRADE REQUIRED" column require manual config upgrade or
resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually
upgrade to is denoted in the "PREFERRED VERSION" column.

API GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED
kubeproxy.config.k8s.io   v1alpha1          v1alpha1            no
kubelet.config.k8s.io     v1beta1           v1beta1             no
_____________________________________________________________________


controlplane ~ ➜  
```

    COMPONENT   CURRENT       TARGET
    kubelet     2 x v1.26.0   v1.26.14

#### We will be upgrading the controlplane node first. Drain the controlplane node of workloads and mark it UnSchedulable


    Controlplane Node: SchedulingDisabled

```ruby
controlplane ~ ➜  kubectl get pod -o wide 
NAME                   READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
blue-987f68cb5-bdfnr   1/1     Running   0          21m   10.244.0.5   controlplane   <none>           <none>
blue-987f68cb5-f6d4k   1/1     Running   0          21m   10.244.1.3   node01         <none>           <none>
blue-987f68cb5-jzcm8   1/1     Running   0          21m   10.244.1.4   node01         <none>           <none>
blue-987f68cb5-mg46s   1/1     Running   0          21m   10.244.0.4   controlplane   <none>           <none>
blue-987f68cb5-znjq7   1/1     Running   0          21m   10.244.1.2   node01         <none>           <none>

controlplane ~ ➜  kubectl get nodes 
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   90m   v1.26.0
node01         Ready    <none>          90m   v1.26.0

controlplane ~ ➜  kubectl drain controlplane --ignore-daemonsets 
node/controlplane cordoned
Warning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-rdqtf, kube-system/kube-proxy-9nbd6
evicting pod kube-system/coredns-787d4945fb-k529n
evicting pod default/blue-987f68cb5-mg46s
evicting pod default/blue-987f68cb5-bdfnr
evicting pod kube-system/coredns-787d4945fb-889q7
pod/blue-987f68cb5-bdfnr evicted
pod/blue-987f68cb5-mg46s evicted
pod/coredns-787d4945fb-k529n evicted
pod/coredns-787d4945fb-889q7 evicted
node/controlplane drained

controlplane ~ ➜  kubectl get pod -o wide 
NAME                   READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
blue-987f68cb5-f6d4k   1/1     Running   0          23m   10.244.1.3    node01   <none>           <none>
blue-987f68cb5-j5tvj   1/1     Running   0          21s   10.244.1.13   node01   <none>           <none>
blue-987f68cb5-jzcm8   1/1     Running   0          23m   10.244.1.4    node01   <none>           <none>
blue-987f68cb5-znjq7   1/1     Running   0          23m   10.244.1.2    node01   <none>           <none>
blue-987f68cb5-zp9fk   1/1     Running   0          22s   10.244.1.11   node01   <none>           <none>

controlplane ~ ➜  kubectl get nodes 
NAME           STATUS                     ROLES           AGE   VERSION
controlplane   Ready,SchedulingDisabled   control-plane   92m   v1.26.0
node01         Ready                      <none>          92m   v1.26.0

controlplane ~ ➜  
```

    There are daemonsets created in this cluster, especially in the kube-system namespace. To ignore these objects and drain the node, we can make use of the --ignore-daemonsets flag.


### Upgrade the ```controlplane``` components to exact version ```v1.27.0```


Upgrade the kubeadm tool (if not already), then the controlplane components, and finally the kubelet. Practice referring to the Kubernetes documentation page.

Note: While upgrading kubelet, if you hit dependency issues while running the ```apt-get upgrade kubelet``` command, use the ```apt install kubelet=1.27.0-00``` command instead.




    Controlplane Node Upgraded to v1.27.0
    
    Controlplane Kubelet Upgraded to v1.27.0

Make sure that the correct version of ```kubeadm``` is installed and then proceed to upgrade the ```controlplane``` node. Once this is done, upgrade the``` kubelet``` on the node.


On the controlplane node, run the following commands:


This will update the package lists from the software repository.

    apt update


This will install the kubeadm version 1.27.0

    apt-get install kubeadm=1.27.0-00


This will upgrade Kubernetes controlplane node.

    kubeadm upgrade apply v1.27.0


Note that the above steps can take a few minutes to complete.



This will update the kubelet with the version 1.27.0.

    apt-get install kubelet=1.27.0-00 


You may need to reload the daemon and restart kubelet service after it has been upgraded.

    systemctl daemon-reload
    systemctl restart kubelet


```ruby
controlplane ~ ➜  apt-get install kubeadm=1.27.0-00
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages will be upgraded:
  kubeadm
1 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.
Need to get 9,931 kB of archives.
After this operation, 1,393 kB of additional disk space will be used.
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 1.27.0-00 [9,931 kB]
Fetched 9,931 kB in 0s (44.5 MB/s)
debconf: delaying package configuration, since apt-utils is not installed
(Reading database ... 20439 files and directories currently installed.)
Preparing to unpack .../kubeadm_1.27.0-00_amd64.deb ...
Unpacking kubeadm (1.27.0-00) over (1.26.0-00) ...
Setting up kubeadm (1.27.0-00) ...

controlplane ~ ➜  kubeadm upgrade apply v1.27.0
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade/version] You have chosen to change the cluster version to "v1.27.0"
[upgrade/versions] Cluster version: v1.26.0
[upgrade/versions] kubeadm version: v1.27.0
[upgrade] Are you sure you want to proceed? [y/N]: y
[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster
[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection
[upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'
W0215 03:41:57.194961   24280 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.0, falling back to the nearest etcd version (3.5.7-0)
W0215 03:42:05.999058   24280 checks.go:835] detected that the sandbox image "k8s.gcr.io/pause:3.6" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.27.0" (timeout: 5m0s)...
[upgrade/etcd] Upgrading to TLS for etcd
W0215 03:42:14.453669   24280 staticpods.go:305] [upgrade/etcd] could not find officially supported version of etcd for Kubernetes v1.27.0, falling back to the nearest etcd version (3.5.7-0)
W0215 03:42:14.457886   24280 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.0, falling back to the nearest etcd version (3.5.7-0)
[upgrade/staticpods] Preparing for "etcd" upgrade
[upgrade/staticpods] Renewing etcd-server certificate
[upgrade/staticpods] Renewing etcd-peer certificate
[upgrade/staticpods] Renewing etcd-healthcheck-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/etcd.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-02-15-03-42-14/etcd.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
[apiclient] Found 1 Pods for label selector component=etcd
[upgrade/staticpods] Component "etcd" upgraded successfully!
[upgrade/etcd] Waiting for etcd to become available
[upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests3006428475"
[upgrade/staticpods] Preparing for "kube-apiserver" upgrade
[upgrade/staticpods] Renewing apiserver certificate
[upgrade/staticpods] Renewing apiserver-kubelet-client certificate
[upgrade/staticpods] Renewing front-proxy-client certificate
[upgrade/staticpods] Renewing apiserver-etcd-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-02-15-03-42-14/kube-apiserver.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
[apiclient] Found 1 Pods for label selector component=kube-apiserver
[upgrade/staticpods] Component "kube-apiserver" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-controller-manager" upgrade
[upgrade/staticpods] Renewing controller-manager.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-controller-manager.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-02-15-03-42-14/kube-controller-manager.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
[apiclient] Found 1 Pods for label selector component=kube-controller-manager
[upgrade/staticpods] Component "kube-controller-manager" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-scheduler" upgrade
[upgrade/staticpods] Renewing scheduler.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-scheduler.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-02-15-03-42-14/kube-scheduler.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
[apiclient] Found 1 Pods for label selector component=kube-scheduler
[upgrade/staticpods] Component "kube-scheduler" upgraded successfully!
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config3138992876/config.yaml
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.27.0". Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.

controlplane ~ ➜  kubeadm upgrade apply v1.27.0
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade/version] You have chosen to change the cluster version to "v1.27.0"
[upgrade/versions] Cluster version: v1.27.0
[upgrade/versions] kubeadm version: v1.27.0
[upgrade] Are you sure you want to proceed? [y/N]: y
[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster
[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection
[upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'
W0215 03:45:40.506718   27080 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.0, falling back to the nearest etcd version (3.5.7-0)
W0215 03:45:40.804655   27080 checks.go:835] detected that the sandbox image "k8s.gcr.io/pause:3.6" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.27.0" (timeout: 5m0s)...
[upgrade/etcd] Upgrading to TLS for etcd
W0215 03:45:41.304960   27080 staticpods.go:305] [upgrade/etcd] could not find officially supported version of etcd for Kubernetes v1.27.0, falling back to the nearest etcd version (3.5.7-0)
W0215 03:45:41.308237   27080 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.0, falling back to the nearest etcd version (3.5.7-0)
[upgrade/staticpods] Preparing for "etcd" upgrade
[upgrade/staticpods] Current and new manifests of etcd are equal, skipping upgrade
[upgrade/etcd] Waiting for etcd to become available
[upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests1869446058"
[upgrade/staticpods] Preparing for "kube-apiserver" upgrade
[upgrade/staticpods] Current and new manifests of kube-apiserver are equal, skipping upgrade
[upgrade/staticpods] Preparing for "kube-controller-manager" upgrade
[upgrade/staticpods] Current and new manifests of kube-controller-manager are equal, skipping upgrade
[upgrade/staticpods] Preparing for "kube-scheduler" upgrade
[upgrade/staticpods] Current and new manifests of kube-scheduler are equal, skipping upgrade
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config2023763105/config.yaml
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.27.0". Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.

controlplane ~ ➜  apt-get install kubelet=1.27.0-00 
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages will be upgraded:
  kubelet
1 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.
Need to get 18.8 MB of archives.
After this operation, 15.1 MB disk space will be freed.
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 1.27.0-00 [18.8 MB]
Fetched 18.8 MB in 0s (39.7 MB/s)
debconf: delaying package configuration, since apt-utils is not installed
(Reading database ... 20439 files and directories currently installed.)
Preparing to unpack .../kubelet_1.27.0-00_amd64.deb ...
/usr/sbin/policy-rc.d returned 101, not running 'stop kubelet.service'
Unpacking kubelet (1.27.0-00) over (1.26.0-00) ...
Setting up kubelet (1.27.0-00) ...
/usr/sbin/policy-rc.d returned 101, not running 'start kubelet.service'

controlplane ~ ➜  systemctl daemon-reload

controlplane ~ ➜  systemctl restart kubelet

```

### Mark the controlplane node as ```"Schedulable"``` again

Run the command: 

    kubectl uncordon controlplane

```ruby
controlplane ~ ➜  kubectl uncordon controlplane 
node/controlplane uncordoned

controlplane ~ ➜  kubectl get nodes
NAME           STATUS   ROLES           AGE    VERSION
controlplane   Ready    control-plane   109m   v1.27.0
node01         Ready    <none>          109m   v1.26.0

controlplane ~ ➜  kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
blue-987f68cb5-f6d4k   1/1     Running   0          40m   10.244.1.3    node01   <none>           <none>
blue-987f68cb5-j5tvj   1/1     Running   0          17m   10.244.1.13   node01   <none>           <none>
blue-987f68cb5-jzcm8   1/1     Running   0          40m   10.244.1.4    node01   <none>           <none>
blue-987f68cb5-znjq7   1/1     Running   0          40m   10.244.1.2    node01   <none>           <none>
blue-987f68cb5-zp9fk   1/1     Running   0          17m   10.244.1.11   node01   <none>           <none>

controlplane ~ ➜  
```


### Next is the worker node. ```Drain``` the worker node of the workloads and mark it ```UnSchedulable```

    Worker node: Unschedulable

```ruby
controlplane ~ ➜  kubectl get nodes
NAME           STATUS   ROLES           AGE    VERSION
controlplane   Ready    control-plane   109m   v1.27.0
node01         Ready    <none>          109m   v1.26.0

controlplane ~ ➜  kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
blue-987f68cb5-f6d4k   1/1     Running   0          40m   10.244.1.3    node01   <none>           <none>
blue-987f68cb5-j5tvj   1/1     Running   0          17m   10.244.1.13   node01   <none>           <none>
blue-987f68cb5-jzcm8   1/1     Running   0          40m   10.244.1.4    node01   <none>           <none>
blue-987f68cb5-znjq7   1/1     Running   0          40m   10.244.1.2    node01   <none>           <none>
blue-987f68cb5-zp9fk   1/1     Running   0          17m   10.244.1.11   node01   <none>           <none>

controlplane ~ ➜  kubectl drain node01 --ignore-daemonsets
node/node01 cordoned
Warning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-xt2hd, kube-system/kube-proxy-r89mp
evicting pod default/blue-987f68cb5-j5tvj
evicting pod kube-system/coredns-5d78c9869d-b7cpb
evicting pod default/blue-987f68cb5-znjq7
evicting pod default/blue-987f68cb5-jzcm8
evicting pod default/blue-987f68cb5-f6d4k
evicting pod kube-system/coredns-5d78c9869d-66bhz
evicting pod default/blue-987f68cb5-zp9fk
pod/blue-987f68cb5-j5tvj evicted
pod/blue-987f68cb5-zp9fk evicted
pod/blue-987f68cb5-znjq7 evicted
pod/blue-987f68cb5-jzcm8 evicted
pod/blue-987f68cb5-f6d4k evicted
pod/coredns-5d78c9869d-b7cpb evicted
pod/coredns-5d78c9869d-66bhz evicted
node/node01 drained

controlplane ~ ➜  kubectl get nodes
NAME           STATUS                     ROLES           AGE    VERSION
controlplane   Ready                      control-plane   112m   v1.27.0
node01         Ready,SchedulingDisabled   <none>          111m   v1.26.0

controlplane ~ ➜  kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP            NODE           NOMINATED NODE   READINESS GATES
blue-987f68cb5-b58gm   1/1     Running   0          18s   10.244.0.10   controlplane   <none>           <none>
blue-987f68cb5-frnjg   1/1     Running   0          18s   10.244.0.6    controlplane   <none>           <none>
blue-987f68cb5-jhd96   1/1     Running   0          18s   10.244.0.9    controlplane   <none>           <none>
blue-987f68cb5-l62tt   1/1     Running   0          18s   10.244.0.8    controlplane   <none>           <none>
blue-987f68cb5-mw2xr   1/1     Running   0          18s   10.244.0.12   controlplane   <none>           <none>

controlplane ~ ➜  
```

#### Upgrade the worker node to the exact version ```v1.27.0```

Make sure that the correct version of ```kubeadm``` is installed and then proceed to upgrade the ```node01``` node. Once this is done, upgrade the ```kubelet``` on the node.


On the node01 node, run the following commands:


If you are on the controlplane node, run ssh node01 to log in to the node01.



This will update the package lists from the software repository.

    apt-get update


This will install the kubeadm version 1.27.0.

    apt-get install kubeadm=1.27.0-00


This will upgrade the node01 configuration.

    kubeadm upgrade node


This will update the kubelet with the version 1.27.0.

    apt-get install kubelet=1.27.0-00 


You may need to reload the daemon and restart the kubelet service after it has been upgraded.

    systemctl daemon-reload
    systemctl restart kubelet


Type exit or logout or enter CTRL + d to go back to the controlplane node.

```ruby
controlplane ~ ➜  ssh node01
Last login: Thu Feb 15 03:57:13 2024 from 192.12.151.13

root@node01 ~ ➜  kubeadm upgrade node
[upgrade] Reading configuration from the cluster...
[upgrade] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks
[preflight] Skipping prepull. Not a control plane node.
[upgrade] Skipping phase. Not a control plane node.
[upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config2717182398/config.yaml
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[upgrade] The configuration for this node was successfully updated!
[upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.

root@node01 ~ ➜  apt-get install kubelet=1.27.0-00
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following held packages will be changed:
  kubelet
The following packages will be upgraded:
  kubelet
1 upgraded, 0 newly installed, 0 to remove and 106 not upgraded.
Need to get 18.8 MB of archives.
After this operation, 15.1 MB disk space will be freed.
Do you want to continue? [Y/n] y
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 1.27.0-00 [18.8 MB]
Fetched 18.8 MB in 0s (61.8 MB/s)
debconf: delaying package configuration, since apt-utils is not installed
(Reading database ... 14818 files and directories currently installed.)
Preparing to unpack .../kubelet_1.27.0-00_amd64.deb ...
/usr/sbin/policy-rc.d returned 101, not running 'stop kubelet.service'
Unpacking kubelet (1.27.0-00) over (1.26.0-00) ...
Setting up kubelet (1.27.0-00) ...
/usr/sbin/policy-rc.d returned 101, not running 'start kubelet.service'

root@node01 ~ ➜  systemctl daemon-reload

root@node01 ~ ➜  systemctl restart kubelet

root@node01 ~ ➜

```

#### Remove the restriction and mark the worker node as schedulable again.

    Worker Node: Schedulable


Exit from Node01

```ruby
controlplane ~ ➜  kubectl uncordon node01
node/node01 uncordoned

controlplane ~ ➜  kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP            NODE           NOMINATED NODE   READINESS GATES
blue-987f68cb5-b58gm   1/1     Running   0          10m   10.244.0.10   controlplane   <none>           <none>
blue-987f68cb5-frnjg   1/1     Running   0          10m   10.244.0.6    controlplane   <none>           <none>
blue-987f68cb5-jhd96   1/1     Running   0          10m   10.244.0.9    controlplane   <none>           <none>
blue-987f68cb5-l62tt   1/1     Running   0          10m   10.244.0.8    controlplane   <none>           <none>
blue-987f68cb5-mw2xr   1/1     Running   0          10m   10.244.0.12   controlplane   <none>           <none>

controlplane ~ ➜  kubectl get nodes
NAME           STATUS   ROLES           AGE    VERSION
controlplane   Ready    control-plane   122m   v1.27.0
node01         Ready    <none>          122m   v1.27.0

```
