### We have deployed a Kubernetes cluster. From the controlplane, verify that how many nodes this cluster has?

    controlplane ~ ➜  kubectl get nodes
    NAME           STATUS   ROLES                  AGE     VERSION
    controlplane   Ready    control-plane,master   3h38m   v1.24.1+k3s1

### How many pods are currently deployed under the default namespace?

Run below command to see how many pods exist in default namespace:


    kubectl get pod -n default



As of now, there are no PODs running under the default namespace.

### Install helm v3.10.3 on controlplane node.


Run below commands:


    wget https://get.helm.sh/helm-v3.10.3-linux-amd64.tar.gz
    tar -zxvf helm-v3.10.3-linux-amd64.tar.gz
    mv linux-amd64/helm /usr/local/bin/
    rm -rf helm-v3.10.3-linux-amd64.tar.gz linux-amd64

```ruby

controlplane ~ ➜  wget https://get.helm.sh/helm-v3.10.3-linux-amd64.tar.gz
--2024-01-27 12:26:39--  https://get.helm.sh/helm-v3.10.3-linux-amd64.tar.gz
Resolving get.helm.sh (get.helm.sh)... 152.195.19.97, 2606:2800:11f:1cb7:261b:1f9c:2074:3c
Connecting to get.helm.sh (get.helm.sh)|152.195.19.97|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 14569235 (14M) [application/x-tar]
Saving to: 'helm-v3.10.3-linux-amd64.tar.gz'

helm-v3.10.3-linux-amd6 100%[==============================>]  13.89M  --.-KB/s    in 0.1s    

2024-01-27 12:26:39 (108 MB/s) - 'helm-v3.10.3-linux-amd64.tar.gz' saved [14569235/14569235]


controlplane ~ ➜  tar -zxvf helm-v3.10.3-linux-amd64.tar.gz
linux-amd64/
linux-amd64/helm
linux-amd64/LICENSE
linux-amd64/README.md

controlplane ~ ➜  mv linux-amd64/helm /usr/local/bin/

controlplane ~ ➜  rm -rf helm-v3.10.3-linux-amd64.tar.gz linux-amd64
```


### We would like to use Prometheus to monitor our Kubernetes cluster as well as the applications that are deployed in this cluster, we want to deploy the Prometheus server on this cluster using the prometheus-community/kube-prometheus-stack helm chart. Remember to name it as prometheus.



Click on PrometheusHelmChart button to get some more details about this chart.

https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack

Note: Due to a known bug you might need to patch the Prometheus node exporter DaemonSet using below command.


    kubectl patch ds prometheus-prometheus-node-exporter --type "json" -p '[{"op": "remove", "path" : "/spec/template/spec/containers/0/volumeMounts/2/mountPropagation"}]'




---

Run below commands:


    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm repo update
    helm install prometheus prometheus-community/kube-prometheus-stack
    kubectl get ds
    kubectl patch ds prometheus-prometheus-node-exporter --type "json" -p '[{"op": "remove", "path" : "/spec/template/spec/containers/0/volumeMounts/2/mountPropagation"}]'


```ruby
controlplane ~ ➜  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
"prometheus-community" has been added to your repositories

controlplane ~ ➜  helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "prometheus-community" chart repository
Update Complete. ⎈Happy Helming!⎈

controlplane ~ ➜  helm install prometheus prometheus-community/kube-prometheus-stack
NAME: prometheus
LAST DEPLOYED: Sat Jan 27 12:31:15 2024
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace default get pods -l "release=prometheus"

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.

controlplane ~ ➜  kubectl get ds
NAME                                  DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
prometheus-prometheus-node-exporter   1         1         0       1            0           kubernetes.io/os=linux   30s

controlplane ~ ➜  kubectl patch ds prometheus-prometheus-node-exporter --type "json" -p '[{"op": "remove", "path" : "/spec/template/spec/containers/0/volumeMounts/2/mountPropagation"}]'
daemonset.apps/prometheus-prometheus-node-exporter patched

```
The provided commands are related to deploying Prometheus and making a specific modification to the configuration of the Prometheus Node Exporter in a Kubernetes cluster. Let me break down the commands for you:

1. **Add Prometheus Community Helm Chart Repository:**
   ```bash
   helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
   ```
   This command adds the Prometheus Community Helm Chart repository to your Helm configuration. Helm is a package manager for Kubernetes, and Helm Charts are pre-configured packages of Kubernetes resources.

2. **Update Helm Repositories:**
   ```bash
   helm repo update
   ```
   This command updates the local Helm chart repositories to ensure you have the latest information about available charts.

3. **Install Prometheus using Helm:**
   ```bash
   helm install prometheus prometheus-community/kube-prometheus-stack
   ```
   This command installs Prometheus and related monitoring components in your Kubernetes cluster using the Helm chart named `kube-prometheus-stack` from the Prometheus Community repository. The release name is set to "prometheus."

4. **Get DaemonSets (ds):**
   ```bash
   kubectl get ds
   ```
   This command lists all DaemonSets in the Kubernetes cluster. DaemonSets ensure that a specific pod runs on all or some nodes in the cluster.

5. **Patch Prometheus Node Exporter DaemonSet:**
   ```bash
   kubectl patch ds prometheus-prometheus-node-exporter --type "json" -p '[{"op": "remove", "path" : "/spec/template/spec/containers/0/volumeMounts/2/mountPropagation"}]'
   ```
   This command patches the configuration of the DaemonSet named `prometheus-prometheus-node-exporter`. It removes a specific mountPropagation setting from the configuration. Mount propagation controls how file system mounts are shared between containers in a pod. This modification is likely done to customize the behavior of the Prometheus Node Exporter.

In summary, these commands deploy Prometheus for monitoring in a Kubernetes cluster using Helm, and the last command modifies the configuration of the Prometheus Node Exporter DaemonSet by removing a mountPropagation setting.

### How many deployments are there in the cluster under default namespace?

```ruby
controlplane ~ ➜  kubectl get deploy
NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
prometheus-kube-prometheus-operator   1/1     1            1           2m52s
prometheus-kube-state-metrics         1/1     1            1           2m52s
prometheus-grafana                    1/1     1            1           2m52s

controlplane ~ ➜

controlplane ~ ➜  kubectl get all
NAME                                                         READY   STATUS    RESTARTS   AGE
pod/prometheus-kube-prometheus-operator-864c68b494-g7ggm     1/1     Running   0          3m51s
pod/prometheus-kube-state-metrics-7686645fcf-cpgwp           1/1     Running   0          3m51s
pod/prometheus-prometheus-node-exporter-s5jfq                1/1     Running   0          3m28s
pod/alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          3m36s
pod/prometheus-grafana-75959bc988-5tvvz                      3/3     Running   0          3m51s
pod/prometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          3m34s

NAME                                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
service/kubernetes                                ClusterIP   10.43.0.1       <none>        443/TCP                      65m
service/prometheus-kube-prometheus-alertmanager   ClusterIP   10.43.159.52    <none>        9093/TCP,8080/TCP            3m53s
service/prometheus-kube-prometheus-operator       ClusterIP   10.43.157.196   <none>        443/TCP                      3m53s
service/prometheus-prometheus-node-exporter       ClusterIP   10.43.199.110   <none>        9100/TCP                     3m53s
service/prometheus-kube-state-metrics             ClusterIP   10.43.10.38     <none>        8080/TCP                     3m53s
service/prometheus-grafana                        ClusterIP   10.43.150.25    <none>        80/TCP                       3m53s
service/prometheus-kube-prometheus-prometheus     ClusterIP   10.43.224.155   <none>        9090/TCP,8080/TCP            3m53s
service/alertmanager-operated                     ClusterIP   None            <none>        9093/TCP,9094/TCP,9094/UDP   3m36s
service/prometheus-operated                       ClusterIP   None            <none>        9090/TCP                     3m34s

NAME                                                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/prometheus-prometheus-node-exporter   1         1         1       1            1           kubernetes.io/os=linux   3m53s

NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/prometheus-kube-prometheus-operator   1/1     1            1           3m53s
deployment.apps/prometheus-kube-state-metrics         1/1     1            1           3m53s
deployment.apps/prometheus-grafana                    1/1     1            1           3m53s

NAME                                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/prometheus-kube-prometheus-operator-864c68b494   1         1         1       3m52s
replicaset.apps/prometheus-kube-state-metrics-7686645fcf         1         1         1       3m52s
replicaset.apps/prometheus-grafana-75959bc988                    1         1         1       3m52s

NAME                                                                    READY   AGE
statefulset.apps/alertmanager-prometheus-kube-prometheus-alertmanager   1/1     3m36s
statefulset.apps/prometheus-prometheus-kube-prometheus-prometheus       1/1     3m34s

controlplane ~ ➜
 
```

#### There are 3 deployments:


1. prometheus-grafana - This is the grafana instance that gets deployed with the helm chart.


2. prometheus-kube-prometheus-operator - Responsible for deploying & managing the prometheus instance.


3. prometheus-kube-state-metrics - Collects cluster level metrics (pods, deployments, etc).

### How many statefulsets have been deployed by the helm chart?


```ruby
NAME                                                                    READY   AGE
statefulset.apps/alertmanager-prometheus-kube-prometheus-alertmanager   1/1     4m46s
statefulset.apps/prometheus-prometheus-kube-prometheus-prometheus       1/1     4m44s


```

### There are two statefulsets that have been deployed by the helm chart:


1. prometheus-prometheus-kube-prometheus-prometheus - This is the prometheus instance.


2. alertmanager-prometheus-kube-prometheus-alertmanager - This is the alertmanager instance.



### What type of services Prometheus helm chart created?


Run the below command:


kubectl get svc



There must be ClusterIP services.

```ruby
controlplane ~ ➜  kubectl get svc
NAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
kubernetes                                ClusterIP   10.43.0.1       <none>        443/TCP                      3h53m
prometheus-kube-prometheus-prometheus     ClusterIP   10.43.13.200    <none>        9090/TCP,8080/TCP            6m54s
prometheus-prometheus-node-exporter       ClusterIP   10.43.136.73    <none>        9100/TCP                     6m54s
prometheus-kube-prometheus-operator       ClusterIP   10.43.156.235   <none>        443/TCP                      6m53s
prometheus-grafana                        ClusterIP   10.43.169.42    <none>        80/TCP                       6m53s
prometheus-kube-state-metrics             ClusterIP   10.43.226.90    <none>        8080/TCP                     6m53s
prometheus-kube-prometheus-alertmanager   ClusterIP   10.43.54.14     <none>        9093/TCP,8080/TCP            6m53s
alertmanager-operated                     ClusterIP   None            <none>        9093/TCP,9094/TCP,9094/UDP   6m41s
prometheus-operated                       ClusterIP   None            <none>        9090/TCP                     6m39s
```

### The prometheus instance is using a ClusterIP service called prometheus-kube-prometheus-prometheus, which means it is only accessible within the cluster, and we can’t access the UI by default.



Update prometheus-kube-prometheus-prometheus service to change it to NodePort service, also make sure to use node port 30111. Once done, you should be able to access the Prometheus UI using Prometheus button on the top bar.



Run the below command:


kubectl edit svc prometheus-kube-prometheus-prometheus



Make some changes in this service. Change type: ClusterIP to type: NodePort and add nodePort: 30111 under - name: http-web. Finally save the changes.

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/db709444-d07f-45a4-805a-64d810aede87)

### Now, configure an ingress to forward traffic to the prometheus service. We already have a template file /root/ingress.yaml with the ingress configuration.


Using this configuration, an ingress called prom-ingress will be created, and there is one routing rule where the host field will match any traffic destined to prometheus.kk-demo.com URL and will forward it to prometheus-kube-prometheus-prometheus service in the backend.



Once done, you should be able to test it using below command:


curl prometheus.kk-demo.com



The output should be something like:


<a href="/graph">Found</a>.




To deploy /root/ingress.yaml template, run the below command:


    kubectl apply -f /root/ingress.yaml


```ruby
controlplane ~ ➜  cat /root/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: prom-ingress
  annotations:
    # kubernetes.io/ingress.class: nginx
spec:
  rules:
    - host: prometheus.kk-demo.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: prometheus-kube-prometheus-prometheus
                port:
                  number: 9090

controlplane ~ ➜  kubectl apply -f /root/ingress.yaml
ingress.networking.k8s.io/prom-ingress created

controlplane ~ ➜  curl prometheus.kk-demo.com
<a href="/graph">Found</a>.

```
### The prometheus instance is already setup to scrape various Kubernetes components including apiserver, coredns, kube-controller-manager, etcd, kube-proxy, kube-scheduler, kubelet(cAdvisor), kube-state-metrics etc.


In addition, it’s setup to scrape the node_exporters as well which are running on Kubernetes node as the prometheus/alertmanager instances and the prometheus operator.



Access the Prometheus UI and navigate to the targets page. how many targets you are seeing there?


Note: On Targets page, click on Collapse All to collapse the details of targets in case you find the list too lengthy.
Click on Prometheus button to access the Prometheus UI and click on Status -> Targets , you would see 12 targets there

![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/9490bc3d-13aa-4ef2-8971-58d3d8826c20)


### Let’s now deploy an application to the Kubernetes cluster. There is an API built with NodeJS that listens on port 3000. It already has an instrumentation setup and the metrics can be accessed at /swagger-stats/metrics. It has two endpoints /recipes, and /ingredients.



Use /root/api-deploy.yaml file to deploy this API in our Kubernetes cluster.


Note: Wait for the PODs to be in a running state before submitting the question.

```ruby
controlplane ~ ➜  cat /root/api-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-deployment
  labels:
    app: api
spec:
  replicas: 2
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      containers:
        - name: api
          image: sanjeevkt720/prometheus-lab-demo
          ports:
            - containerPort: 3000

controlplane ~ ➜  kubectl apply -f /root/api-deploy.yaml
deployment.apps/api-deployment created

controlplane ~ ➜  kubectl get all
NAME                                                         READY   STATUS    RESTARTS   AGE
pod/prometheus-kube-prometheus-operator-864c68b494-g7ggm     1/1     Running   0          32m
pod/prometheus-kube-state-metrics-7686645fcf-cpgwp           1/1     Running   0          32m
pod/prometheus-prometheus-node-exporter-s5jfq                1/1     Running   0          32m
pod/alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          32m
pod/prometheus-grafana-75959bc988-5tvvz                      3/3     Running   0          32m
pod/prometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          32m
pod/api-deployment-5d59ffc885-lgg2t                          1/1     Running   0          48s
pod/api-deployment-5d59ffc885-gt2hf                          1/1     Running   0          48s

NAME                                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
service/kubernetes                                ClusterIP   10.43.0.1       <none>        443/TCP                         94m
service/prometheus-kube-prometheus-alertmanager   ClusterIP   10.43.159.52    <none>        9093/TCP,8080/TCP               32m
service/prometheus-kube-prometheus-operator       ClusterIP   10.43.157.196   <none>        443/TCP                         32m
service/prometheus-prometheus-node-exporter       ClusterIP   10.43.199.110   <none>        9100/TCP                        32m
service/prometheus-kube-state-metrics             ClusterIP   10.43.10.38     <none>        8080/TCP                        32m
service/prometheus-grafana                        ClusterIP   10.43.150.25    <none>        80/TCP                          32m
service/alertmanager-operated                     ClusterIP   None            <none>        9093/TCP,9094/TCP,9094/UDP      32m
service/prometheus-operated                       ClusterIP   None            <none>        9090/TCP                        32m
service/prometheus-kube-prometheus-prometheus     NodePort    10.43.224.155   <none>        9090:30111/TCP,8080:31827/TCP   32m

NAME                                                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/prometheus-prometheus-node-exporter   1         1         1       1            1           kubernetes.io/os=linux   32m

NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/prometheus-kube-prometheus-operator   1/1     1            1           32m
deployment.apps/prometheus-kube-state-metrics         1/1     1            1           32m
deployment.apps/prometheus-grafana                    1/1     1            1           32m
deployment.apps/api-deployment                        2/2     2            2           49s

NAME                                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/prometheus-kube-prometheus-operator-864c68b494   1         1         1       32m
replicaset.apps/prometheus-kube-state-metrics-7686645fcf         1         1         1       32m
replicaset.apps/prometheus-grafana-75959bc988                    1         1         1       32m
replicaset.apps/api-deployment-5d59ffc885                        2         2         2       49s

NAME                                                                    READY   AGE
statefulset.apps/alertmanager-prometheus-kube-prometheus-alertmanager   1/1     32m
statefulset.apps/prometheus-prometheus-kube-prometheus-prometheus       1/1     32m

controlplane ~ ➜  
```

### Since API is already deployed, let’s deploy a service to provide reachability to the API. There is a file called /root/api-service.yaml that has a service configuration. Deploy the API service using this definition file.

```ruby
controlplane ~ ➜  cat /root/api-service.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: api-service
  labels:
    job: node-api
    app: api
spec:
  type: NodePort
  selector:
    app: api
  ports:
    - name: web
      protocol: TCP
      port: 3000
      targetPort: 3000
      nodePort: 30112

controlplane ~ ➜  kubectl apply -f /root/api-service.yaml
service/api-service created

controlplane ~ ➜  kubectl get svc
NAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
kubernetes                                ClusterIP   10.43.0.1       <none>        443/TCP                         97m
prometheus-kube-prometheus-alertmanager   ClusterIP   10.43.159.52    <none>        9093/TCP,8080/TCP               35m
prometheus-kube-prometheus-operator       ClusterIP   10.43.157.196   <none>        443/TCP                         35m
prometheus-prometheus-node-exporter       ClusterIP   10.43.199.110   <none>        9100/TCP                        35m
prometheus-kube-state-metrics             ClusterIP   10.43.10.38     <none>        8080/TCP                        35m
prometheus-grafana                        ClusterIP   10.43.150.25    <none>        80/TCP                          35m
alertmanager-operated                     ClusterIP   None            <none>        9093/TCP,9094/TCP,9094/UDP      35m
prometheus-operated                       ClusterIP   None            <none>        9090/TCP                        35m
prometheus-kube-prometheus-prometheus     NodePort    10.43.224.155   <none>        9090:30111/TCP,8080:31827/TCP   35m
api-service                               NodePort    10.43.135.168   <none>        3000:30112/TCP                  19s

controlplane ~ ➜  
```
### Let’s update prom-ingress ingress to setup a rule to route any traffic to the host api.kk-demo.com to the API service.

Edit prom-ingress ingress:


    kubectl edit ingress prom-ingress



Under rules: add the below rule in it and save:


      - host: api.kk-demo.com
        http:
          paths:
          - backend:
              service:
                name: api-service
                port:
                  number: 3000
            path: /
            pathType: Prefix


![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/a8249021-f59f-4df3-b998-c2bd83007fe2)

## Now that the API is installed, we need to configure Prometheus to scrape the API pods. Instead of manually going into the prometheus configs and adding another scrape config, with the prometheus operator, we can declaratively define all the endpoints prometheus should scrape by creating a ServiceMonitor. Prometheus will automatically pickup all targets to scrape from the defined ServiceMonitors.


ServiceMonitors are a Custom Resource Definition provided by the Prometheus Operator.


Run a kubectl get crd to see all of the CRDs provided by the Prometheus Operator.

If you would like to take a look at all of the ServiceMonitors that have been deployed, run kubectl get serviceMonitor. You can see a separate ServiceMonitor for each of the targets prometheus has already been configured to scrape.



There’s an api-servicemonitor.yaml file with the base configuration for a ServiceMonitor, it has some PLACEHOLDER which needs to be updated before you apply this template. Let’s look at what's configured there and what you should update.


a. kind is set to ServiceMonitor.


b. selector.matchLabels - This is a selector label to tell prometheus which service to scrape. We will need to match the label of our api-service.


c. spec.endpoints - has scrape configurations for prometheus.


        1. interval - This is equivalent to scrape_interval.

        2. port - This is going to reference the name of the specific port in our service.

        3. path - This is the specific path that will expose metrics in the container, equivalent to metrics_path config.


Once the template is updated, let's deploy the same.

```ruby
controlplane ~ ➜  kubectl get crd
NAME                                        CREATED AT
addons.k3s.cattle.io                        2024-01-28T02:47:29Z
helmcharts.helm.cattle.io                   2024-01-28T02:47:29Z
helmchartconfigs.helm.cattle.io             2024-01-28T02:47:29Z
traefikservices.traefik.containo.us         2024-01-28T02:48:07Z
middlewaretcps.traefik.containo.us          2024-01-28T02:48:07Z
ingressroutes.traefik.containo.us           2024-01-28T02:48:07Z
ingressrouteudps.traefik.containo.us        2024-01-28T02:48:07Z
tlsoptions.traefik.containo.us              2024-01-28T02:48:07Z
serverstransports.traefik.containo.us       2024-01-28T02:48:07Z
middlewares.traefik.containo.us             2024-01-28T02:48:07Z
tlsstores.traefik.containo.us               2024-01-28T02:48:07Z
ingressroutetcps.traefik.containo.us        2024-01-28T02:48:07Z
alertmanagerconfigs.monitoring.coreos.com   2024-01-28T03:49:05Z
alertmanagers.monitoring.coreos.com         2024-01-28T03:49:05Z
podmonitors.monitoring.coreos.com           2024-01-28T03:49:05Z
probes.monitoring.coreos.com                2024-01-28T03:49:06Z
prometheusagents.monitoring.coreos.com      2024-01-28T03:49:06Z
prometheuses.monitoring.coreos.com          2024-01-28T03:49:08Z
prometheusrules.monitoring.coreos.com       2024-01-28T03:49:08Z
scrapeconfigs.monitoring.coreos.com         2024-01-28T03:49:08Z
servicemonitors.monitoring.coreos.com       2024-01-28T03:49:08Z
thanosrulers.monitoring.coreos.com          2024-01-28T03:49:09Z

controlplane ~ ➜  kubectl get serviceMonitor
NAME                                                 AGE
prometheus-kube-prometheus-kubelet                   43m
prometheus-kube-prometheus-coredns                   43m
prometheus-grafana                                   43m
prometheus-kube-state-metrics                        43m
prometheus-kube-prometheus-kube-controller-manager   43m
prometheus-prometheus-node-exporter                  43m
prometheus-kube-prometheus-kube-proxy                43m
prometheus-kube-prometheus-kube-scheduler            43m
prometheus-kube-prometheus-apiserver                 43m
prometheus-kube-prometheus-kube-etcd                 43m
prometheus-kube-prometheus-operator                  43m
prometheus-kube-prometheus-prometheus                43m
prometheus-kube-prometheus-alertmanager              43m

controlplane ~ ➜  cat /root/api-servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: api-service-monitor
  labels:
    app: prometheus
    #PLACEHOLDER: add label so prometheus will discover this ServiceMonitor
spec:
  jobLabel: job
  endpoints:
    - interval: 30s
      port: web
      path: /swagger-stats/metrics
  selector:
    matchLabels:
      #PLACEHOLDER: add label from api-service

controlplane ~ ➜  cat > /root/api-servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: api-service-monitor
  labels:
    app: prometheus
    release: prometheus
spec:
  jobLabel: job
  endpoints:
    - interval: 30s
      port: web
      path: /swagger-stats/metrics
  selector:
    matchLabels:
      app: api
controlplane ~ ➜  kubectl apply -f api-servicemonitor.yaml
servicemonitor.monitoring.coreos.com/api-service-monitor created

controlplane ~ ➜  
```


### If you go to Prometheus UI now to look for the targets, you must find api-service-monitor under targets. How many Endpoints are successfully being scraped by Prometheus under api-service-monitor target?



Note: Since you just setup ServiceMonitors for API in the previous question so you might need to wait for a few seconds/minutes before api-service-monitor reflects under Prometheus UI.


Click on Prometheus button to access the Prometheus UI and click on Status -> Targets, further click on show more for api-service-monitor, you would see 2 endpoints under it.


![image](https://github.com/Althaf-official/Kodekloud_Learning/assets/105126131/699cb6c2-209b-47b6-914a-5f7a7e746654)



### To add recording-rules and alerts, once again we will not have to touch the prometheus configuration. The prometheus operator comes with another Custom Resource Definition to add rules.


Run kubectl get crd and you should see prometheusrules.monitoring.coreos.com. To create rules, we will define a new prometheus rules object. You can take a look at all of the pre-existing rules by running kubectl get prometheusrules.monitoring.coreos.com command.


There is a /root/rules.yaml with a base configuration to get a simple rule configured. Let’s walkthrough it.


a. kind - Set to PrometheusRule.

b. name - Name of this specific PrometheusRule object.

c. spec - It will contain regular prometheus rule/alert configuration.


Inspect the /root/rules.yaml file and under the spec, we can see a group named node-example being defined and there’s one rule named node_filesystem_free_percent.


The rule configurations are identical to how they are configured in a prometheus.yml file, just place all your groups/rules under the spec portion of the configuration.



Update the PLACEHOLDER under rules: in this template to add a new alert for the node group with the following configs:


a. Alert name should be HostOutOfMemory.

b. Expression should be node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10

c. Labels should be as below:

      1. team: infra

      2. severity: warning


Note: You do not need to apply this template yet.

```ruby
controlplane ~ ➜  cat /root/rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    #PLACEHOLDER: Add label here
  name: example-rules
spec:
  groups:
    - name: node-example
      rules:
        - record: node_filesystem_free_percent
          expr: 100* node_filesystem_free_bytes / node_filesystem_size_bytes
        #PLACEHOLDER: Add alert here


controlplane ~ ➜  cat > /root/rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    #PLACEHOLDER: Add label here
  name: example-rules
spec:
  groups:
    - name: node-example
      rules:
        - record: node_filesystem_free_percent
          expr: 100* node_filesystem_free_bytes / node_filesystem_size_bytes
        - alert: HostOutOfMemory
          expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
          labels:
            team: infra
            severity: warning
controlplane ~ ➜  
```

### Before we apply the /root/rules.yaml template, there is one last modification that needs to be made. Just like the ServiceMonitor, the PrometheusRule objects needs to have a specific label assigned to it so that the Prometheus instance can discover it.



What label will prometheus look for to discover new rules?



To get the label, we’ll need to take a look at the Prometheus CRD configuration in Kubernetes. There’s a config property called ruleSelector which will have the label that the prometheus instance will look for. This can be verified by running the following command:


kubectl get prometheus -o yaml | grep -A 3 -i ruleSelector



You will find release: prometheus under ruleSelector: -> matchLabels:.

### So, as you found in the previous question that Prometheus will look for a label release: prometheus to discover new rules. Update the PLACEHOLDER under labels: in /root/rules.yaml template file and apply the same.


Modify the /root/rules.yaml template as below:


apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    release: prometheus
  name: example-rules
spec:
  groups:
    - name: node-example
      rules:
        - record: node_filesystem_free_percent
          expr: 100* node_filesystem_free_bytes / node_filesystem_size_bytes
        - alert: HostOutOfMemory
          expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
          labels:
            team: infra
            severity: warning



Save and apply the template.


kubectl apply -f /root/rules.yaml 

### Verify the rules were added by checking the Rules page on the Prometheus Web-UI.


Note: Since you just updated the rule in the previous question so you might need to wait for a few seconds/minutes before the rules reflect under Prometheus UI.



#### Let’s now create a new alertmanager rule. The prometheus Operator has an AlertmanagerConfig CRD that is used for adding rules to alertmanager.



The file /root/alertmanager-rule.yaml has a base configuration for applying a new rule. Let’s walkthrough it:


1. kind - AlertmanagerConfig

2. metadata.name - Name of the object.

3. metadata.labels - Labels associated with this object.

4. spec - This is where the standard alertmanager configs are going to be defined.


Currently there is a default route configuration and a single receiver called webhook. Add a route and a receiver in this template:


Route:

    routes:
      - matchers:
          - name: team
            value: infra
        receiver: "infra"
        groupBy: ["severity"]



Receiver:

    - name: "infra"
      webhookConfigs:
        - url: "http://infra.com/"



Finally apply the same.



---

Modify the template /root/alertmanager-rule.yaml as below:


apiVersion: monitoring.coreos.com/v1alpha1
kind: AlertmanagerConfig
metadata:
  name: alert-config
  labels:
    release: prometheus
spec:
  route:
    groupBy: ["alertname"]
    groupWait: 30s
    groupInterval: 5m
    repeatInterval: 12h
    receiver: "webhook"
    routes:
      - matchers:
          - name: team
            value: infra
        receiver: "infra"
        groupBy: ["severity"]
  receivers:
    - name: "webhook"
      webhookConfigs:
        - url: "http://example.com/"

    - name: "infra"
      webhookConfigs:
        - url: "http://infra.com/"



Save and apply the template:


kubectl apply -f /root/alertmanager-rule.yaml


### Let’s set up and access the alert manager UI now.


Let’s update prom-ingress ingress to setup a rule to route traffic destined to alertmanager.kk-demo.com to the alert manager service. The service name is prometheus-kube-prometheus-alertmanager and the port number is 9093



Edit prom-ingress ingress:


    kubectl edit ingress prom-ingress



Under rules: add the below rule in it and save:


      - host: alertmanager.kk-demo.com
        http:
          paths:
          - backend:
              service:
                name: prometheus-kube-prometheus-alertmanager
                port:
                  number: 9093
            path: /
            pathType: Prefix

            
